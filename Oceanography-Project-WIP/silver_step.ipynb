{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from imports import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_user = \"flosrv\"\n",
    "password = \"Nesrine123\"\n",
    "host = \"localhost\"\n",
    "port = 3306\n",
    "database = \"Oceanography_data_analysis\"\n",
    "metadata = MetaData()\n",
    "# Connect to the database\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_user}:{password}@{host}/{database}\", isolation_level ='AUTOCOMMIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour r√©cup√©rer les tables dont le nom commence par un pr√©fixe sp√©cifique\n",
    "def get_tables_starting_with(engine, prefix: str):\n",
    "    inspector = inspect(engine)\n",
    "    all_tables = inspector.get_table_names()\n",
    "    tables_with_prefix = [table for table in all_tables if table.startswith(prefix)]\n",
    "    return tables_with_prefix\n",
    "\n",
    "# Fonction pour r√©cup√©rer les donn√©es de chaque table (en ignorant l'absence de tables)\n",
    "def get_data_from_table(engine, table_name):\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{table_name}`\"  # Utilisation de backticks pour les noms de tables\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des donn√©es pour la table {table_name}: {e}\")\n",
    "        return None  # Retourner None en cas d'erreur\n",
    "    \n",
    "def clean_dataframe(df):\n",
    "    for column in df.columns:\n",
    "        # Calculer le pourcentage de valeurs manquantes\n",
    "        missing_percentage = df[column].isnull().mean() * 100\n",
    "        \n",
    "        # Supprimer la colonne si elle est totalement nulle\n",
    "        if df[column].isnull().sum() == len(df[column]):\n",
    "            df = df.drop(columns=[column])\n",
    "            continue\n",
    "        \n",
    "        # Si plus de 50% des valeurs sont manquantes, on retire la colonne sauf si c'est num√©rique\n",
    "        if missing_percentage > 50:\n",
    "            if df[column].dtype not in ['float64', 'int64']:  # Ne pas supprimer les colonnes num√©riques\n",
    "                df = df.drop(columns=[column])\n",
    "        else:\n",
    "            # Si la colonne est num√©rique, on remplace les NaN par la m√©diane\n",
    "            if df[column].dtype in ['float64', 'int64']:  # v√©rifier si c'est une colonne num√©rique\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "            else:\n",
    "                pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les Donn√©es des Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "# Dictionnaire pour stocker les donn√©es tri√©es par Station ID\n",
    "buoy_datas = {}\n",
    "\n",
    "# R√©cup√©rer les tables commen√ßant par 'br_'\n",
    "bronze_tables = get_tables_starting_with(engine, prefix='br_')\n",
    "\n",
    "# Parcourir les tables et les organiser par Station ID et type (marine ou meteo)\n",
    "for table in bronze_tables:\n",
    "    parts = table.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Format inattendu\n",
    "\n",
    "    station_id = str(parts[1])  # Convertir en string pour √©viter KeyError\n",
    "    label = parts[2].lower()  # \"marine\" ou \"meteo\"\n",
    "\n",
    "    df = get_data_from_table(engine, table)\n",
    "\n",
    "    if station_id not in buoy_datas:\n",
    "        buoy_datas[station_id] = {\"Marine Dataframe\": None, \"Meteo Dataframe\": None}  \n",
    "\n",
    "    if label == \"marine\":\n",
    "        buoy_datas[station_id][\"Marine Dataframe\"] = df\n",
    "    elif label == \"meteo\":\n",
    "        buoy_datas[station_id][\"Meteo Dataframe\"] = df  \n",
    "\n",
    "# V√©rification\n",
    "print(f\"üìä Nombre total de stations au d√©but : {len(buoy_datas)}\")\n",
    "print(\"üìù Stations initiales :\")\n",
    "for station_id in buoy_datas.keys():\n",
    "    print(f\"  - Station ID: {station_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting Missing Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virer les df manquantes\n",
    "stations_removed = 0\n",
    "stations_to_remove = []\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    try:\n",
    "        print(f\"\\nüîç Traitement de la station ID : {station_id}\")\n",
    "\n",
    "        marine_data = data.get(\"Marine Dataframe\")\n",
    "        meteo_data = data.get(\"Meteo Dataframe\")\n",
    "\n",
    "        if marine_data is None or meteo_data is None:\n",
    "            print(f\"‚ö†Ô∏è Dataframe manquante pour la station {station_id}. Suppression.\")\n",
    "            stations_to_remove.append(station_id)\n",
    "            stations_removed += 1\n",
    "            continue \n",
    "\n",
    "        print(f\"‚úÖ Marine Dataframe: {marine_data.shape[0]} lignes\")\n",
    "        print(f\"‚úÖ Meteo Dataframe: {meteo_data.shape[0]} lignes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sur la station {station_id}: {str(e)}\")\n",
    "\n",
    "# Supprimer les stations sans donn√©es\n",
    "for station_id in stations_to_remove:\n",
    "    del buoy_datas[station_id]\n",
    "\n",
    "print(f\"\\nüìä Nombre de stations restantes : {len(buoy_datas)}\")\n",
    "print(f\"üóëÔ∏è Nombre de stations supprim√©es : {stations_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Enrichment with MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merged_rows = 0  # Variable pour compter le nombre total de lignes fusionn√©es\n",
    "\n",
    "################## Data enrichment ##########################################\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nüîç Traitement de la Station ID: {station_id}\")\n",
    "\n",
    "    marine_df = data[\"Marine Dataframe\"]\n",
    "    meteo_df = data[\"Meteo Dataframe\"]\n",
    "\n",
    "    try:\n",
    "        buoy_metadata = get_station_metadata(station_id)\n",
    "        parsed_data = parse_buoy_json(buoy_metadata)\n",
    "\n",
    "        # Ajouter les m√©tadonn√©es dans la DataFrame Marine\n",
    "        if marine_df is not None:\n",
    "            for key, value in parsed_data.items():\n",
    "                marine_df[key] = value \n",
    "\n",
    "        # Mise √† jour du dictionnaire avec les m√©tadonn√©es\n",
    "        data.update(parsed_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de r√©cup√©ration des m√©tadonn√©es pour {station_id}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Marine Dataframe: {marine_df.shape[0]} lignes\" if marine_df is not None else \"‚ö†Ô∏è Marine Dataframe: Aucune donn√©e\")\n",
    "    print(f\"‚úÖ Meteo Dataframe: {meteo_df.shape[0]} lignes\" if meteo_df is not None else \"‚ö†Ô∏è Meteo Dataframe: Aucune donn√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des DataFrames fusionn√©s\n",
    "for station_id, data in buoy_datas.items():\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Nettoyage des donn√©es pour la station {station_id}\")\n",
    "\n",
    "        marine_df = data[\"Marine Dataframe\"]\n",
    "        meteo_df = data[\"Meteo Dataframe\"]\n",
    "\n",
    "        if marine_df is None:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Marine DataFrame manquant)\")\n",
    "            continue\n",
    "        if meteo_df is None:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Meteo DataFrame manquant)\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        try:\n",
    "            cleaned_marine_df = clean_dataframe(marine_df)\n",
    "\n",
    "            # Ajouter le DataFrame nettoy√© au dictionnaire des r√©sultats\n",
    "            buoy_datas[station_id] = {'Cleaned Marine Dataframe': cleaned_marine_df}\n",
    "            print(f\"‚úÖ Nettoyage r√©ussi pour la station {station_id} ({cleaned_marine_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du nettoyage pour {station_id}: {e}\")\n",
    "\n",
    "# R√©sum√© final du nettoyage\n",
    "print(\"\\nüìä R√âSUM√â DU NETTOYAGE:\")\n",
    "print(f\"üìå Stations au d√©part : {len(buoy_datas)}\")\n",
    "print(f\"‚úÖ Stations nettoy√©es : {len([data for data in buoy_datas.values() if 'Cleaned Dataframe' in data])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Try to Merge ################################################################################\n",
    "list_ID =[]\n",
    "list_merged_df =[]\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    list_ID.append(station_id)\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Fusion des DataFrames pour la station {station_id}\")\n",
    "\n",
    "        # V√©rifier si les deux DataFrames existent\n",
    "        if marine_df is None or meteo_df is None:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e (donn√©es manquantes)\")\n",
    "            continue\n",
    "\n",
    "        # Assurez-vous que la colonne 'Datetime' existe dans les deux DataFrames\n",
    "        if 'Datetime' not in marine_df.columns or 'Datetime' not in meteo_df.columns:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e (colonne 'Datetime' manquante)\")\n",
    "            continue\n",
    "\n",
    "        # Assurez-vous que la colonne 'Datetime' est dans le bon format datetime\n",
    "        marine_df['Datetime'] = pd.to_datetime(marine_df['Datetime'], errors='coerce')\n",
    "        meteo_df['Datetime'] = pd.to_datetime(meteo_df['Datetime'], errors='coerce')\n",
    "\n",
    "        # Ajouter le DataFrame fusionn√© au dictionnaire des r√©sultats\n",
    "        try:\n",
    "            # Tenter un merge inner sur la colonne temporelle 'Datetime'\n",
    "            merged_df = marine_df.merge(meteo_df, how=\"inner\", on=\"Datetime\")  # Ajuste 'timestamp' si n√©cessaire\n",
    "            buoy_datas[station_id]['Merged Dataframe'] = merged_df\n",
    "            list_merged_df.append(merged_df)\n",
    "        except Exception as e:\n",
    "            print(f'Error loading Merged Dataframe for Buoy{station_id}')\n",
    "        print(f\"‚úÖ Fusion r√©ussie pour la station {station_id} ({merged_df.shape[0]} lignes)\")\n",
    "\n",
    "        # Ajouter le nombre de lignes de ce DataFrame fusionn√© au total\n",
    "        total_merged_rows += merged_df.shape[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la fusion pour {station_id}: {e}\")\n",
    "\n",
    "# R√©sum√© final de la fusion\n",
    "print(\"\\nüìä R√âSUM√â DE LA FUSION:\")\n",
    "print(f\"üìå Stations au d√©part : {len(buoy_datas)}\")\n",
    "print(f\"üìä Total de lignes fusionn√©es : {total_merged_rows}\")\n",
    "\n",
    "print(f'\\ntest Df:\\n{buoy_datas[\"42058\"]['Merged Dataframe']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{buoy_datas[\"42058\"]['Merged Dataframe'].shape}')\n",
    "print(f'\\n{buoy_datas[\"42058\"]['Merged Dataframe'].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{buoy_datas[\"42058\"]['Cleaned Dataframe'].shape}')\n",
    "print(f'\\n{buoy_datas[\"42058\"]['Cleaned Dataframe'].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marine_cols = [\n",
    "    \"wind_direction\", \"wind_speed\", \"wind_gust\", \"wave_height\",\n",
    "    \"dominant_wave_period\", \"average_wave_period\", \"dominant_wave_direction\",\n",
    "    \"pressure\", \"air_temperature\", \"water_temperature\", \"dewpoint\",\n",
    "    \"visibility\", \"3hr_pressure_tendency\", \"water_level_above_mean\"\n",
    "]\n",
    "\n",
    "meteo_cols = [\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\", \"rain\",\n",
    "    \"showers\", \"pressure_msl\", \"surface_pressure\", \"cloud_cover\", \"cloud_cover_low\",\n",
    "    \"cloud_cover_mid\", \"cloud_cover_high\", \"visibility\", \"wind_speed_10m\",\n",
    "    \"soil_temperature_0cm\", \"soil_moisture_0_to_1cm\"\n",
    "]\n",
    "\n",
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (km)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "meteo_cols_to_delete = ['soil_temperature_0cm','rain', 'showers', 'is_day',\n",
    "                  'soil_moisture_0_to_1cm']\n",
    "\n",
    "for station_id, tables in buoy_datas.items():\n",
    "    marine_df = tables[\"Marine DataFrame\"]\n",
    "    marine_df = rename_columns(marine_df, col_to_rename)\n",
    "\n",
    "    marine_df = drop_columns_if_exist\n",
    "\n",
    "    meteo_df = tables[\"Meteo DataFrame\"]\n",
    "    meteo_df = rename_columns(meteo_df,col_to_rename)\n",
    "    meteo_df = drop_columns_if_exist(meteo_df, meteo_cols_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOUR RESAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling des donn√©es et stockage dans un nouveau compartiment du dictionnaire \n",
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "        print(f\"üîÅ Processing and resampling marine data for station {station_id}...\")\n",
    "        # Convert columns to numeric types (float or int) excluding datetime columns using pandas to_numeric\n",
    "        tables[\"Marine DataFrame\"] = process_datetime_column(tables[\"Marine DataFrame\"], column='time')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Marine Data for {station_id}: {e}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÅ Processing and resampling weather data for station {station_id}...\")\n",
    "        # Convert columns to numeric types (float or int) excluding datetime columns using pandas to_numeric\n",
    "        tables[\"Meteo DataFrame\"] = process_datetime_column(tables[\"Meteo DataFrame\"], column='date')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Meteo Data for {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Adding MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "        df_merged = tables[\"Merged DataFrame\"]\n",
    "        print(f\"üîó Changing Data Types  for station {station_id}...\")\n",
    "        df_converted = convert_df_columns(df_merged)\n",
    "        tables[\"Converted DataFrame\"] = df_converted\n",
    "        \n",
    "        print(f\"Successfully Changed Data Types for Station {station_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error changing data types for station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "\n",
    "        print(f\"üîó Cleaning DataFrame for station {station_id}...\")\n",
    "        df_converted = tables[\"Converted DataFrame\"]\n",
    "        \n",
    "        df_cleaned = clean_dataframe(df_converted)\n",
    "\n",
    "        tables[\"Cleaned DataFrame\"] = df_cleaned\n",
    "\n",
    "        print(f\"Successfully Cleaned DataFrame for Station {station_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Cleaning DataFrame for station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating All in One Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion finale de tous les DataFrames\n",
    "try:\n",
    "    print(\"üîÄ Merging all DataFrames into a final DataFrame...\")\n",
    "    dataframes_to_concat = [tables[\"Cleaned DataFrame\"] for tables in buoys_datas.values()]\n",
    "\n",
    "    df_final = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during final merge: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n‚≠êüèÜ Processing complete!\")\n",
    "print(f\"üî¢ Total stations processed: {len(buoys_datas)}\")\n",
    "\n",
    "if df_final is not None and not df_final.empty:\n",
    "    print(f\"üìù Final merged DataFrame size: {df_final.shape}\")\n",
    "else:\n",
    "    print(\"The DataFrame is either None or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir toutes les colonnes contenant \"Station ID\" dans leur nom\n",
    "for column in df_final.columns:\n",
    "    if \"Station ID\" in column:\n",
    "        try:\n",
    "            # Tenter de convertir la colonne en num√©rique (en utilisant pd.to_numeric avec errors='coerce')\n",
    "            df_final[column] = pd.to_numeric(df_final[column], errors='raise')\n",
    "             # Si la conversion est r√©ussie, convertir en int\n",
    "            df_final[column] = df_final[column].astype(int) \n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"Error in Conversion Step 1 for column: {column}:\\n{e}\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df_final[column] = df_final[column].astype(str)\n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"Error in Conversion Step 2 for column: {column}:\\n{e}\")\n",
    "            \n",
    "\n",
    "show_first_row(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = clean_dataframe(df_final)\n",
    "df_final.isnull().sum()\n",
    "df_final2 = df_final.dropna()\n",
    "print(f'{df_final2.shape}\\n\\n{df_final2.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (km)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "meteo_cols_to_delete = ['soil_temperature_0cm','rain', 'showers', 'id_x','id_y' 'is_day',\n",
    "                  'soil_moisture_0_to_1cm']\n",
    "\n",
    "\n",
    "df_final2 = rename_columns(df_final2,{' Visibility (km)_y':'Visibility (km)'})\n",
    "\n",
    "df_final2['Visibility (km)'] = df_final2['Visibility (km)']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2 = df_final2.round(2)\n",
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2[['Daytime', 'Month']] = df_final2['Datetime'].apply(lambda x: get_day_time(x)).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2=df_final2.round(2)\n",
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming, Dropping Useless Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Envoi Vers PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_in_table(engine=engine, schema = schema_silver, table_name='Silver_Table', df=df_final2, key_column='Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer le dataframe pour la Station ID 42058\n",
    "df_42058 = df_final2[df_final2[\"Station ID\"] == 42058]\n",
    "df_42058.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Variables de contr√¥le des appels API\n",
    "# vc_api_key_path = r\"c:\\Credentials\\visual_crossing_weather_api.json\"\n",
    "# with open(vc_api_key_path, 'r') as file:\n",
    "#     content = json.load(file)\n",
    "#     vc_api_key = content[\"api_key\"]\n",
    "\n",
    "# # Ajouter un index num√©rique automatique (0, 1, 2, ...)\n",
    "# df_42058.index = range(len(df_42058))\n",
    "\n",
    "# # Assurez-vous que lat et lon sont d√©finis\n",
    "# lat, lon = None, None\n",
    "\n",
    "# # Si le DataFrame n'est pas vide, r√©cup√©rer les coordonn√©es de la premi√®re ligne\n",
    "# if not df_42058.empty:\n",
    "#     first_row = df_42058.iloc[0]\n",
    "#     lat = first_row[\"Lat\"]\n",
    "#     lon = first_row[\"Lon\"]\n",
    "\n",
    "#     # Convertir les coordonn√©es (si n√©cessaire)\n",
    "#     lat, lon = convert_coordinates(lat, lon)\n",
    "\n",
    "# print(f'{lat}\\n{lon}\\n{vc_api_key}')\n",
    "\n",
    "# # D√©finition des dates dynamiques\n",
    "# today = datetime.now().strftime(\"%Y-%m-%d\")  # Aujourd'hui\n",
    "# last_month = (datetime.now() - timedelta(days=31)).strftime(\"%Y-%m-%d\")  # 31 jours avant aujourd'hui\n",
    "\n",
    "# # V√©rifier si lat et lon ont √©t√© correctement d√©finis avant de continuer\n",
    "# if lat is not None and lon is not None:\n",
    "#     # Construction de l'URL\n",
    "#     url_last_month = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat},{lon}/{last_month}/{today}?unitGroup=metric&key={vc_api_key}&contentType=json\"\n",
    "    \n",
    "#     # Tentative de r√©cup√©ration des donn√©es m√©t√©o\n",
    "#     try:\n",
    "#         response = requests.get(url_last_month)\n",
    "        \n",
    "#         # V√©rifier si la requ√™te a r√©ussi (code 200)\n",
    "#         if response.status_code == 200:\n",
    "#             vc_meteo_data = response.json()  # Essayer de d√©coder le JSON\n",
    "#             print(f\"Donn√©es m√©t√©o r√©cup√©r√©es : {vc_meteo_data}\")\n",
    "#         else:\n",
    "#             print(f\"Erreur lors de l'appel √† l'API, code de statut : {response.status_code}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de la r√©cup√©ration des donn√©es m√©t√©o : {e}\")\n",
    "# else:\n",
    "#     print(\"Les coordonn√©es (lat, lon) ne sont pas d√©finies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming the df_cleaned DataFrame already exists and contains the required data\n",
    "\n",
    "# # First, load your Visual Crossing Weather Data (example, you may already have it)\n",
    "# # Assuming vc_meteo_data is the JSON response from Visual Crossing\n",
    "# # Example of flattening the JSON\n",
    "# df_vc_meteo = pd.json_normalize(vc_meteo_data, record_path=[\"days\", \"hours\"], meta=[\"days\"])\n",
    "\n",
    "# # Convert the datetimeEpoch from Visual Crossing Weather data into Date and Hour columns\n",
    "# df_vc_meteo[\"Date\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d\")\n",
    "# df_vc_meteo[\"Hour\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%H\")\n",
    "\n",
    "# # Filter data from df_vc_meteo for the last 30 days\n",
    "# today = datetime.now()\n",
    "# thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "# today_str = today.strftime(\"%Y-%m-%d\")\n",
    "# thirty_days_ago_str = thirty_days_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# # Filter df_vc_meteo for the last 30 days\n",
    "# df_test_last_month = df_vc_meteo[['Date', 'Hour', 'windspeed']]\n",
    "# df_test_last_month = df_test_last_month[(df_test_last_month['Date'] >= thirty_days_ago_str) & \n",
    "#                                         (df_test_last_month['Date'] <= today_str)]\n",
    "\n",
    "# # Prepare df_cleaned for merging (add Date and Hour columns)\n",
    "# df_cleaned['Date'] = df_cleaned['Datetime'].dt.strftime(\"%Y-%m-%d\")\n",
    "# df_cleaned['Hour'] = df_cleaned['Datetime'].dt.strftime(\"%H\")\n",
    "\n",
    "# # Filter df_cleaned for the last 30 days\n",
    "# df_cleaned_last_month = df_cleaned[(df_cleaned['Date'] >= thirty_days_ago_str) & \n",
    "#                                    (df_cleaned['Date'] <= today_str)]\n",
    "\n",
    "# # Merge df_vc_meteo and df_cleaned based on Date and Hour\n",
    "# df_merged = df_test_last_month.merge(df_cleaned_last_month[['Date', 'Hour', 'Wind Speed (km/h)', 'wind_speed_10m']], \n",
    "#                                     on=['Date', 'Hour'], \n",
    "#                                     how='inner')\n",
    "\n",
    "# # Display the merged dataframe\n",
    "# print(df_merged.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (%)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "df_cleaned = rename_columns(df_cleaned, col_to_rename)\n",
    "df_cleaned = drop_columns_if_exist(df_cleaned,['soil_temperature_0cm','rain', 'showers', 'is_day', 'id_x', 'id_y','soil_moisture_0_to_1cm'])\n",
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  R√©cup√©rer les donn√©es de l'API\n",
    "# vc_meteo_data = response.json()\n",
    "# print(vc_meteo_data)  # V√©rifiez les donn√©es r√©cup√©r√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normaliser les donn√©es JSON en DataFrame\n",
    "# df_vc_meteo = pd.json_normalize(vc_meteo_data, record_path=[\"days\", \"hours\"], meta=[\"days\"])\n",
    "\n",
    "# # Afficher la premi√®re ligne des donn√©es\n",
    "# df_vc_meteo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du timestamp en datetime\n",
    "df_vc_meteo[\"Date\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d\")\n",
    "df_vc_meteo[\"Hour\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir les dates de filtrage pour les 30 derniers jours\n",
    "today = datetime.now()\n",
    "thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "# Convertir les dates en format YYYY-MM-DD\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "thirty_days_ago_str = thirty_days_ago.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les donn√©es des 30 derniers jours de df_vc_meteo\n",
    "df_test_last_month = df_vc_meteo[['Date', 'Hour', 'windspeed']]\n",
    "df_test_last_month = df_test_last_month[(df_test_last_month['Date'] >= thirty_days_ago_str) & \n",
    "                                        (df_test_last_month['Date'] <= today_str)]\n",
    "\n",
    "# Ajouter les colonnes Date et Hour √† df_42058\n",
    "df_42058.loc[:, 'Date'] = df_42058['Datetime'].dt.strftime(\"%Y-%m-%d\")\n",
    "df_42058.loc[:, 'Hour'] = df_42058['Datetime'].dt.strftime(\"%H\")\n",
    "\n",
    "# Filtrer les donn√©es des 30 derniers jours dans df_42058\n",
    "df_42058_last_month = df_42058[(df_42058['Date'] >= thirty_days_ago_str) & \n",
    "                                (df_42058['Date'] <= today_str)]\n",
    "\n",
    "# Fusionner les deux DataFrames sur Date et Hour\n",
    "df_test_merged = df_test_last_month.merge(df_42058_last_month[['Date', 'Hour', 'Wind Speed (km/h)', 'wind_speed_10m']], \n",
    "                                     on=['Date', 'Hour'], \n",
    "                                     how='inner')\n",
    "\n",
    "df_test_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def handle_null_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     row_count = df.shape[0]\n",
    "    \n",
    "#     # Initialisation des listes pour suivre les colonnes supprim√©es\n",
    "#     removed_columns = []\n",
    "#     non_numeric_columns_to_drop = []\n",
    "    \n",
    "#     # Utiliser lambda et apply() pour calculer le nombre de valeurs nulles dans chaque colonne\n",
    "#     null_counts = df.apply(lambda col: int(col.isnull().sum()))  # Calculer le nombre de NaN par colonne\n",
    "    \n",
    "#     # Condition : 1. Colonnes avec toutes les valeurs nulles ou 2. Plus de 50% de valeurs nulles et colonne non num√©rique\n",
    "#     columns_to_drop = null_counts[\n",
    "#         (null_counts == row_count) | \n",
    "#         ((null_counts > row_count * 0.5) & ~df.apply(lambda col: pd.api.types.is_numeric_dtype(col)))\n",
    "#     ].index\n",
    "    \n",
    "#     # Ajouter les noms des colonnes supprim√©es dans les listes appropri√©es\n",
    "#     for col in columns_to_drop:\n",
    "#         if null_counts[col] == row_count:\n",
    "#             removed_columns.append(col)  # Colonnes enti√®rement vides\n",
    "#         elif null_counts[col] > row_count * 0.5 and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             non_numeric_columns_to_drop.append(col)  # Colonnes > 50% nulles et non num√©riques\n",
    "    \n",
    "#     # Supprimer les colonnes identifi√©es\n",
    "#     df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "#     # Afficher les r√©sultats\n",
    "#     print(\"Colonnes supprim√©es pour avoir toutes les valeurs nulles:\")\n",
    "#     print(removed_columns)\n",
    "    \n",
    "#     print(\"\\nColonnes supprim√©es pour avoir plus de 50% de valeurs nulles et √™tre non num√©riques:\")\n",
    "#     print(non_numeric_columns_to_drop)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# # df_final = pd.read_csv('ton_fichier.csv') # Assure-toi que df_final est bien un DataFrame valide avant d'appeler la fonction\n",
    "# df_final = handle_null_values(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df_final.round(2)\n",
    "# print(df_final.columns)\n",
    "# df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explore_dict_keys(d, parent_key='', sep='_'):\n",
    "#     \"\"\"\n",
    "#     Explore un dictionnaire r√©cursivement pour obtenir toutes les cl√©s, y compris les sous-cl√©s,\n",
    "#     mais ne retourne pas les valeurs finales.\n",
    "\n",
    "#     :param d: Le dictionnaire √† explorer\n",
    "#     :param parent_key: La cl√© parent qui est utilis√©e pour concat√©ner les sous-cl√©s\n",
    "#     :param sep: Le s√©parateur utilis√© pour concat√©ner les cl√©s (par d√©faut '_')\n",
    "#     :return: Une liste des cl√©s (et sous-cl√©s)\n",
    "#     \"\"\"\n",
    "#     keys = []\n",
    "#     for k, v in d.items():\n",
    "#         new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "#         if isinstance(v, dict):  # Si la valeur est un dictionnaire, on explore r√©cursivement\n",
    "#             keys.append(new_key)  # Ajouter la cl√©, mais ne pas inclure la valeur\n",
    "#             keys.extend(explore_dict_keys(v, new_key, sep=sep))  # Continuer l'exploration\n",
    "#         else:\n",
    "#             keys.append(new_key)  # Ajouter la cl√© finale\n",
    "#     return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_key_path(d, target_key, path=[]):\n",
    "#     \"\"\"\n",
    "#     Recherche r√©cursive d'une cl√© dans un dictionnaire et retourne son chemin.\n",
    "#     :param d: dictionnaire\n",
    "#     :param target_key: cl√© recherch√©e\n",
    "#     :param path: liste pour stocker le chemin jusqu'√† la cl√©\n",
    "#     :return: chemin sous forme de liste\n",
    "#     \"\"\"\n",
    "#     if isinstance(d, dict):  # Si le dictionnaire est encore imbriqu√©\n",
    "#         for key, value in d.items():\n",
    "#             new_path = path + [key]\n",
    "#             if key == target_key:\n",
    "#                 return new_path\n",
    "#             elif isinstance(value, dict):\n",
    "#                 result = find_key_path(value, target_key, new_path)\n",
    "#                 if result:  # Si la cl√© est trouv√©e, retourner le chemin\n",
    "#                     return result\n",
    "#     return None  # Retourne None si la cl√© n'a pas √©t√© trouv√©e\n",
    "\n",
    "\n",
    "\n",
    "# # Recherche du chemin pour la cl√© 'marine_data'\n",
    "# path = find_key_path(table_dict, \"Marine Dataframe\")\n",
    "# print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto_convert Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (buoy_id, tables) in enumerate(table_dict.items()):  # Utilisation de .items() pour obtenir (cl√©, valeur)\n",
    "#     if isinstance(tables, dict):\n",
    "#         if idx == 1:  # V√©rifier si l'index est √©gal √† 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Rows of all Dataframes in total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
