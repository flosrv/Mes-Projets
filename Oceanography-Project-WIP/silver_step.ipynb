{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from imports import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_user = \"flosrv\"\n",
    "password = \"Nesrine123\"\n",
    "host = \"localhost\"\n",
    "port = 3306\n",
    "database = \"Oceanography_data_analysis\"\n",
    "metadata = MetaData()\n",
    "# Connect to the database\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_user}:{password}@{host}/{database}\", isolation_level ='AUTOCOMMIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les Donn√©es des Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(mysql.connector.errors.ProgrammingError) 1146 (42S02): Table 'oceanography_data_analysis.br_44027_marine_jonesport' doesn't exist\n[SQL: SELECT * FROM br_44027_marine_jonesport,_me]\n(Background on this error at: https://sqlalche.me/e/20/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMySQLInterfaceError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:755\u001b[39m, in \u001b[36mCMySQLConnection.cmd_query\u001b[39m\u001b[34m(self, query, raw, buffered, raw_as_string, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m         query = query.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmysql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffered\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mMySQLInterfaceError\u001b[39m: Table 'oceanography_data_analysis.br_44027_marine_jonesport' doesn't exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1964\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:945\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py:351\u001b[39m, in \u001b[36mCMySQLCursor.execute\u001b[39m\u001b[34m(self, operation, params, map_results)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_result(\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcmd_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stmt_partition\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmappable_stmt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffered\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     )\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py:97\u001b[39m, in \u001b[36mwith_context_propagation.<locals>.wrapper\u001b[39m\u001b[34m(cnx, *args, **kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OTEL_ENABLED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cnx.otel_context_propagation:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m current_span = trace.get_current_span()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:763\u001b[39m, in \u001b[36mCMySQLConnection.cmd_query\u001b[39m\u001b[34m(self, query, raw, buffered, raw_as_string, **kwargs)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[32m    764\u001b[39m         err.errno, msg=err.msg, sqlstate=err.sqlstate\n\u001b[32m    765\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mProgrammingError\u001b[39m: 1146 (42S02): Table 'oceanography_data_analysis.br_44027_marine_jonesport' doesn't exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m label = parts[\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# Marine ou Meteo est la partie apr√®s le deuxi√®me '_'\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# R√©cup√©rer les donn√©es de la table\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m df = \u001b[43mget_data_from_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Si le dictionnaire n'a pas encore de cl√© pour cette station_id, on l'ajoute\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m station_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stations_data:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_data_from_table\u001b[39m\u001b[34m(engine, table_name)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data_from_table\u001b[39m(engine, table_name):\n\u001b[32m     12\u001b[39m     query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\pandas\\io\\sql.py:734\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql.read_table(\n\u001b[32m    725\u001b[39m         sql,\n\u001b[32m    726\u001b[39m         index_col=index_col,\n\u001b[32m   (...)\u001b[39m\u001b[32m    731\u001b[39m         dtype_backend=dtype_backend,\n\u001b[32m    732\u001b[39m     )\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\pandas\\io\\sql.py:1836\u001b[39m, in \u001b[36mSQLDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1781\u001b[39m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1788\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1789\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m   1790\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[33;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[32m   1792\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1834\u001b[39m \n\u001b[32m   1835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1836\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1837\u001b[39m     columns = result.keys()\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\pandas\\io\\sql.py:1659\u001b[39m, in \u001b[36mSQLDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   1657\u001b[39m args = [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[32m   1658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1659\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.con.execute(sql, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1776\u001b[39m, in \u001b[36mConnection.exec_driver_sql\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1771\u001b[39m execution_options = \u001b[38;5;28mself\u001b[39m._execution_options.merge_with(\n\u001b[32m   1772\u001b[39m     execution_options\n\u001b[32m   1773\u001b[39m )\n\u001b[32m   1775\u001b[39m dialect = \u001b[38;5;28mself\u001b[39m.dialect\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1843\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_insertmany_context(dialect, context)\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1983\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1980\u001b[39m     result = context._setup_result_proxy()\n\u001b[32m   1982\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1983\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2352\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2350\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2351\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1964\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1962\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n\u001b[32m   1969\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_cursor_execute(\n\u001b[32m   1970\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1971\u001b[39m         cursor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1975\u001b[39m         context.executemany,\n\u001b[32m   1976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:945\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py:351\u001b[39m, in \u001b[36mCMySQLCursor.execute\u001b[39m\u001b[34m(self, operation, params, map_results)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m._executed = (\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m._stmt_partition[\u001b[33m\"\u001b[39m\u001b[33msingle_stmts\u001b[39m\u001b[33m\"\u001b[39m].popleft()\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_results\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stmt_partition[\u001b[33m\"\u001b[39m\u001b[33mmappable_stmt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    347\u001b[39m )\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_result(\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcmd_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stmt_partition\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmappable_stmt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffered\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraw_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     )\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[32m    360\u001b[39m         msg=err.msg, errno=err.errno, sqlstate=err.sqlstate\n\u001b[32m    361\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py:97\u001b[39m, in \u001b[36mwith_context_propagation.<locals>.wrapper\u001b[39m\u001b[34m(cnx, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# pylint: disable=possibly-used-before-assignment\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OTEL_ENABLED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cnx.otel_context_propagation:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m current_span = trace.get_current_span()\n\u001b[32m    100\u001b[39m tp_header = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Oceanography-Project-WIP\\env\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:763\u001b[39m, in \u001b[36mCMySQLConnection.cmd_query\u001b[39m\u001b[34m(self, query, raw, buffered, raw_as_string, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m     \u001b[38;5;28mself\u001b[39m._cmysql.query(\n\u001b[32m    756\u001b[39m         query,\n\u001b[32m    757\u001b[39m         raw=raw,\n\u001b[32m   (...)\u001b[39m\u001b[32m    760\u001b[39m         query_attrs=\u001b[38;5;28mself\u001b[39m.query_attrs,\n\u001b[32m    761\u001b[39m     )\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[32m    764\u001b[39m         err.errno, msg=err.msg, sqlstate=err.sqlstate\n\u001b[32m    765\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    767\u001b[39m     addr = (\n\u001b[32m    768\u001b[39m         \u001b[38;5;28mself\u001b[39m._unix_socket \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._unix_socket \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._port\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    769\u001b[39m     )\n",
      "\u001b[31mProgrammingError\u001b[39m: (mysql.connector.errors.ProgrammingError) 1146 (42S02): Table 'oceanography_data_analysis.br_44027_marine_jonesport' doesn't exist\n[SQL: SELECT * FROM br_44027_marine_jonesport,_me]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "# Fonction pour r√©cup√©rer toutes les tables br_\n",
    "# Fonction pour r√©cup√©rer les donn√©es de chaque table\n",
    "def get_data_from_table(engine, table_name):\n",
    "    # V√©rification de la syntaxe de la requ√™te SQL\n",
    "    query = f\"SELECT * FROM `{table_name}`\"  # Utilisation des guillemets pour le nom de la table\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction pour r√©cup√©rer les donn√©es de chaque table\n",
    "def get_data_from_table(engine, table_name):\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "\n",
    "# Dictionnaire pour stocker les donn√©es tri√©es par Station ID\n",
    "stations_data = {}\n",
    "\n",
    "# R√©cup√©rer les noms de toutes les tables commen√ßant par 'br_'\n",
    "bronze_tables = get__tables_starting_by(engine, start='br_')\n",
    "\n",
    "# Parcourir les tables et les organiser par Station ID et type (marine ou meteo)\n",
    "for table in bronze_tables:\n",
    "    # Extraire l'ID de la station et le label (marine ou meteo)\n",
    "    parts = table.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Si le nom de la table ne suit pas le format attendu, ignorer\n",
    "    \n",
    "    station_id = parts[1]  # Station ID est la partie apr√®s le premier '_'\n",
    "    label = parts[2]  # Marine ou Meteo est la partie apr√®s le deuxi√®me '_'\n",
    "    \n",
    "    # R√©cup√©rer les donn√©es de la table\n",
    "    df = get_data_from_table(engine, table)\n",
    "    \n",
    "    # Si le dictionnaire n'a pas encore de cl√© pour cette station_id, on l'ajoute\n",
    "    if station_id not in stations_data:\n",
    "        stations_data[station_id] = {'marine': None, 'meteo': None}\n",
    "    \n",
    "    # Stocker les donn√©es dans la cl√© appropri√©e (marine ou meteo)\n",
    "    if label == 'marine':\n",
    "        stations_data[station_id]['marine'] = df\n",
    "    elif label == 'meteo':\n",
    "        stations_data[station_id]['meteo'] = df\n",
    "\n",
    "# R√©sum√© des donn√©es collect√©es\n",
    "for station_id, data in stations_data.items():\n",
    "    print(f\"\\nStation ID: {station_id}\")\n",
    "    if data['marine'] is not None:\n",
    "        print(f\"  Marine Data: {data['marine'].shape[0]} rows\")\n",
    "    else:\n",
    "        print(f\"  Marine Data: Aucune donn√©e disponible\")\n",
    "    \n",
    "    if data['meteo'] is not None:\n",
    "        print(f\"  Meteo Data: {data['meteo'].shape[0]} rows\")\n",
    "    else:\n",
    "        print(f\"  Meteo Data: Aucune donn√©e disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buoys_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Importation des modules n√©cessaires\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd\n",
    "\n",
    "# üìå Connexion √† la base de donn√©es MySQL\n",
    "mysql_user = \"flosrv\"\n",
    "password = \"Nesrine123\"\n",
    "host = \"localhost\"\n",
    "database = \"Oceanography_data_analysis\"\n",
    "\n",
    "# Cr√©ation de l'objet engine pour interagir avec la base MySQL\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_user}:{password}@{host}/{database}\", isolation_level='AUTOCOMMIT')\n",
    "\n",
    "# üìå R√©cup√©ration des noms de tables contenant \"bronze\"\n",
    "inspector = inspect(engine)\n",
    "table_names = [t for t in inspector.get_table_names() if \"br_\" in t.lower()]\n",
    "print(f\"üî¢ Nombre total de tables dans le sch√©ma : {len(table_names)}\\n\")\n",
    "\n",
    "# üìå Filtrage des tables selon leur cat√©gorie\n",
    "marine_tables = {t for t in table_names if \"marine\" in t.lower()}\n",
    "meteo_tables = {t for t in table_names if \"meteo\" in t.lower()}\n",
    "buoys_data_table = {t for t in table_names if \"buoy\" in t.lower()}\n",
    "\n",
    "print(f\"üåä Tables marines trouv√©es : {len(marine_tables)}\")\n",
    "print(f\"üåßÔ∏è Tables m√©t√©o trouv√©es : {len(meteo_tables)}\")\n",
    "print(f\"üêã Tables de bou√©es trouv√©es : {len(buoys_data_table)}\\n\")\n",
    "\n",
    "# üìå Fonction pour r√©cup√©rer les donn√©es d'une table SQL et les convertir en DataFrame\n",
    "def fetch_table_data(engine, table_name, schema=None):\n",
    "    query = f\"SELECT * FROM {schema + '.' if schema else ''}{table_name}\"\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "# üìå Chargement des donn√©es de bou√©es (si une table correspondante existe)\n",
    "if buoys_data_table:\n",
    "    print(\"üîÑ Chargement des donn√©es de la table 'buoys_datas'...\")\n",
    "    try:\n",
    "        # Prendre la premi√®re table trouv√©e dans la liste des tables de bou√©es\n",
    "        table_name = next(iter(buoys_data_table))\n",
    "        df_buoy = fetch_table_data(engine, table_name)\n",
    "\n",
    "        if not df_buoy.empty:\n",
    "            print(\"üì¶ Donn√©es r√©cup√©r√©es pour 'buoys_datas'.\")\n",
    "\n",
    "            # V√©rifier si \"Station ID\" est bien une colonne\n",
    "            if \"Station ID\" in df_buoy.columns:\n",
    "                # üìå Transformer la DataFrame en dictionnaire index√© par \"Station ID\"\n",
    "                buoys_datas = df_buoy.set_index(\"Station ID\").to_dict(orient=\"index\")\n",
    "                print(f\"‚úÖ Table 'buoys_datas' charg√©e avec succ√®s ! Nombre de bou√©es : {len(df_buoy)}\\n\")\n",
    "            else:\n",
    "                print(\"‚ùå ERREUR : 'Station ID' introuvable dans la table des bou√©es !\")\n",
    "                buoys_datas = {}\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Aucun r√©sultat trouv√© dans 'buoys_datas'.\\n\")\n",
    "            buoys_datas = {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement de 'buoys_datas': {e}\\n\")\n",
    "        buoys_datas = {}\n",
    "\n",
    "# üìå Chargement des donn√©es Marine et M√©t√©o et mise √† jour du dictionnaire buoys_datas\n",
    "for table_set, label, icon in [\n",
    "    (marine_tables, \"Marine\", \"üåä\"),\n",
    "    (meteo_tables, \"Meteo\", \"üåßÔ∏è\")\n",
    "]:\n",
    "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "    for table_name in table_set:\n",
    "        print(f\"üîÑ Chargement des donn√©es pour la table {label} : {table_name}...\")\n",
    "\n",
    "        try:\n",
    "            df_data = fetch_table_data(engine, table_name)\n",
    "\n",
    "            if not df_data.empty:\n",
    "                print(f\"üì¶ Donn√©es r√©cup√©r√©es pour {table_name}.\")\n",
    "\n",
    "                # üìå Extraire le Station ID depuis le nom de la table (ex: \"bronze_marine_123\")\n",
    "                station_id = table_name.split(\"_\")[-1]\n",
    "\n",
    "                # V√©rifier si la station existe d√©j√† dans buoys_datas\n",
    "                if station_id in buoys_datas:\n",
    "                    # Ajouter les donn√©es m√©t√©o ou marines au dictionnaire\n",
    "                    buoys_datas[station_id][f\"{label} DataFrame\"] = df_data.to_dict(orient=\"records\")\n",
    "                    print(f\"{icon} Donn√©es {label} charg√©es pour {table_name} ! Nombre de lignes collect√©es : {len(df_data)}\\n\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Station ID {station_id} introuvable dans buoys_datas, donn√©es ignor√©es.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Aucun r√©sultat trouv√© pour {table_name}.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors du chargement des donn√©es {label} pour {table_name} : {e}\\n\")\n",
    "\n",
    "# üìå R√©capitulatif final\n",
    "total_tables = len(table_names)\n",
    "total_marine = len(marine_tables)\n",
    "total_meteo = len(meteo_tables)\n",
    "\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(f\"üìä Statistiques finales :\")\n",
    "print(f\"üî¢ Nombre total de tables : {total_tables}\")\n",
    "print(f\"üåä Nombre de tables marines : {total_marine}\")\n",
    "print(f\"üåßÔ∏è Nombre de tables m√©t√©o : {total_meteo}\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "# üìå Fermeture propre de la connexion MySQL\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoys_datas[\"42058\"][\"Marine DataFrame\"].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoys_datas[\"42058\"][\"Meteo DataFrame\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoys_datas[\"42058\"][\"Meteo DataFrame\"].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_silver_merged_df = []  \n",
    "list_failed_dfs = []        \n",
    "\n",
    "number_marine_data = 0\n",
    "number_meteo_data = 0\n",
    "number_merged_data = 0\n",
    "\n",
    "marine_data_conversion = 0\n",
    "meteo_data_conversion = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marine_cols = [\n",
    "    \"wind_direction\", \"wind_speed\", \"wind_gust\", \"wave_height\",\n",
    "    \"dominant_wave_period\", \"average_wave_period\", \"dominant_wave_direction\",\n",
    "    \"pressure\", \"air_temperature\", \"water_temperature\", \"dewpoint\",\n",
    "    \"visibility\", \"3hr_pressure_tendency\", \"water_level_above_mean\"\n",
    "]\n",
    "\n",
    "meteo_cols = [\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\", \"rain\",\n",
    "    \"showers\", \"pressure_msl\", \"surface_pressure\", \"cloud_cover\", \"cloud_cover_low\",\n",
    "    \"cloud_cover_mid\", \"cloud_cover_high\", \"visibility\", \"wind_speed_10m\",\n",
    "    \"soil_temperature_0cm\", \"soil_moisture_0_to_1cm\"\n",
    "]\n",
    "\n",
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (km)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "meteo_cols_to_delete = ['soil_temperature_0cm','rain', 'showers', 'is_day',\n",
    "                  'soil_moisture_0_to_1cm']\n",
    "\n",
    "for station_id, tables in buoys_datas.items():\n",
    "    marine_df = tables[\"Marine DataFrame\"]\n",
    "    marine_df = rename_columns(marine_df, col_to_rename)\n",
    "\n",
    "    marine_df = drop_columns_if_exist\n",
    "\n",
    "    meteo_df = tables[\"Meteo DataFrame\"]\n",
    "    meteo_df = rename_columns(meteo_df,col_to_rename)\n",
    "    meteo_df = drop_columns_if_exist(meteo_df, meteo_cols_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOUR RESAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling des donn√©es et stockage dans un nouveau compartiment du dictionnaire \n",
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "        print(f\"üîÅ Processing and resampling marine data for station {station_id}...\")\n",
    "        # Convert columns to numeric types (float or int) excluding datetime columns using pandas to_numeric\n",
    "        tables[\"Marine DataFrame\"] = process_datetime_column(tables[\"Marine DataFrame\"], column='time')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Marine Data for {station_id}: {e}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÅ Processing and resampling weather data for station {station_id}...\")\n",
    "        # Convert columns to numeric types (float or int) excluding datetime columns using pandas to_numeric\n",
    "        tables[\"Meteo DataFrame\"] = process_datetime_column(tables[\"Meteo DataFrame\"], column='date')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Meteo Data for {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des coordonn√©es\n",
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "        tables[\"Marine DataFrame\"][\"Lat\"] = tables[\"Lat\"]\n",
    "        tables[\"Marine DataFrame\"][\"Lon\"] = tables[\"Lon\"]\n",
    "        print(f\"üåê Coordinates (Lat/Lon) added for station {station_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding coordinates for {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Based on Station ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_merged_data = 0\n",
    "list_silver_merged_df=[]\n",
    "# Fusion des DataFrames\n",
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "\n",
    "        metadatas = get_station_metadata(station_id=station_id)\n",
    "        print(f\"üîó Merging marine and weather data for station {station_id}...\")\n",
    "        df_merged = pd.merge(\n",
    "            tables[\"Marine DataFrame\"], tables[\"Meteo DataFrame\"], on='Datetime', how='inner'\n",
    "        )\n",
    "        tables[\"Merged DataFrame\"] = df_merged\n",
    "        number_merged_data += df_merged.shape[0]\n",
    "        list_silver_merged_df.append(df_merged)\n",
    "    except Exception as e:\n",
    "        print(f\"Error merging data for station {station_id}: {e}\")\n",
    "\n",
    "# print(f'{len(list_silver_merged_df)} DataFrames Merged :\\n{number_merged_data} rows in total !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in buoys_datas.items():\n",
    "    if value['id'] == 1:\n",
    "        for key, val in value.items():\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_first_row(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Adding MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des coordonn√©es\n",
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "\n",
    "        df_merged = tables[\"Merged DataFrame\"]\n",
    "        station_id = str(df_merged[\"Station ID\"])\n",
    "        print(station_id)\n",
    "        metadatas = get_station_metadata(station_id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error adding Metadata for {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "        df_merged = tables[\"Merged DataFrame\"]\n",
    "        print(f\"üîó Changing Data Types  for station {station_id}...\")\n",
    "        df_converted = convert_df_columns(df_merged)\n",
    "        tables[\"Converted DataFrame\"] = df_converted\n",
    "        \n",
    "        print(f\"Successfully Changed Data Types for Station {station_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Error changing data types for station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    for column in df.columns:\n",
    "        # Calculer le pourcentage de valeurs manquantes\n",
    "        missing_percentage = df[column].isnull().mean() * 100\n",
    "        \n",
    "        # Supprimer la colonne si elle est totalement nulle\n",
    "        if df[column].isnull().sum() == len(df[column]):\n",
    "            df = df.drop(columns=[column])\n",
    "            continue\n",
    "        \n",
    "        # Si plus de 50% des valeurs sont manquantes, on retire la colonne sauf si c'est num√©rique\n",
    "        if missing_percentage > 50:\n",
    "            if df[column].dtype not in ['float64', 'int64']:  # Ne pas supprimer les colonnes num√©riques\n",
    "                df = df.drop(columns=[column])\n",
    "        else:\n",
    "            # Si la colonne est num√©rique, on remplace les NaN par la m√©diane\n",
    "            if df[column].dtype in ['float64', 'int64']:  # v√©rifier si c'est une colonne num√©rique\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "            else:\n",
    "                pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_id, tables in buoys_datas.items():\n",
    "    try:\n",
    "\n",
    "        print(f\"üîó Cleaning DataFrame for station {station_id}...\")\n",
    "        df_converted = tables[\"Converted DataFrame\"]\n",
    "        \n",
    "        df_cleaned = clean_dataframe(df_converted)\n",
    "\n",
    "        tables[\"Cleaned DataFrame\"] = df_cleaned\n",
    "\n",
    "        print(f\"Successfully Cleaned DataFrame for Station {station_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Cleaning DataFrame for station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating All in One Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion finale de tous les DataFrames\n",
    "try:\n",
    "    print(\"üîÄ Merging all DataFrames into a final DataFrame...\")\n",
    "    dataframes_to_concat = [tables[\"Cleaned DataFrame\"] for tables in buoys_datas.values()]\n",
    "\n",
    "    df_final = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during final merge: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n‚≠êüèÜ Processing complete!\")\n",
    "print(f\"üî¢ Total stations processed: {len(buoys_datas)}\")\n",
    "\n",
    "if df_final is not None and not df_final.empty:\n",
    "    print(f\"üìù Final merged DataFrame size: {df_final.shape}\")\n",
    "else:\n",
    "    print(\"The DataFrame is either None or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir toutes les colonnes contenant \"Station ID\" dans leur nom\n",
    "for column in df_final.columns:\n",
    "    if \"Station ID\" in column:\n",
    "        try:\n",
    "            # Tenter de convertir la colonne en num√©rique (en utilisant pd.to_numeric avec errors='coerce')\n",
    "            df_final[column] = pd.to_numeric(df_final[column], errors='raise')\n",
    "             # Si la conversion est r√©ussie, convertir en int\n",
    "            df_final[column] = df_final[column].astype(int) \n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"Error in Conversion Step 1 for column: {column}:\\n{e}\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df_final[column] = df_final[column].astype(str)\n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"Error in Conversion Step 2 for column: {column}:\\n{e}\")\n",
    "            \n",
    "\n",
    "show_first_row(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = clean_dataframe(df_final)\n",
    "df_final.isnull().sum()\n",
    "df_final2 = df_final.dropna()\n",
    "print(f'{df_final2.shape}\\n\\n{df_final2.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (km)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "meteo_cols_to_delete = ['soil_temperature_0cm','rain', 'showers', 'id_x','id_y' 'is_day',\n",
    "                  'soil_moisture_0_to_1cm']\n",
    "\n",
    "\n",
    "df_final2 = rename_columns(df_final2,{' Visibility (km)_y':'Visibility (km)'})\n",
    "\n",
    "df_final2['Visibility (km)'] = df_final2['Visibility (km)']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2 = df_final2.round(2)\n",
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2[['Daytime', 'Month']] = df_final2['Datetime'].apply(lambda x: get_day_time(x)).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2=df_final2.round(2)\n",
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming, Dropping Useless Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_first_row(df_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Envoi Vers PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_in_table(engine=engine, schema = schema_silver, table_name='Silver_Table', df=df_final2, key_column='Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer le dataframe pour la Station ID 42058\n",
    "df_42058 = df_final2[df_final2[\"Station ID\"] == 42058]\n",
    "df_42058.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Variables de contr√¥le des appels API\n",
    "# vc_api_key_path = r\"c:\\Credentials\\visual_crossing_weather_api.json\"\n",
    "# with open(vc_api_key_path, 'r') as file:\n",
    "#     content = json.load(file)\n",
    "#     vc_api_key = content[\"api_key\"]\n",
    "\n",
    "# # Ajouter un index num√©rique automatique (0, 1, 2, ...)\n",
    "# df_42058.index = range(len(df_42058))\n",
    "\n",
    "# # Assurez-vous que lat et lon sont d√©finis\n",
    "# lat, lon = None, None\n",
    "\n",
    "# # Si le DataFrame n'est pas vide, r√©cup√©rer les coordonn√©es de la premi√®re ligne\n",
    "# if not df_42058.empty:\n",
    "#     first_row = df_42058.iloc[0]\n",
    "#     lat = first_row[\"Lat\"]\n",
    "#     lon = first_row[\"Lon\"]\n",
    "\n",
    "#     # Convertir les coordonn√©es (si n√©cessaire)\n",
    "#     lat, lon = convert_coordinates(lat, lon)\n",
    "\n",
    "# print(f'{lat}\\n{lon}\\n{vc_api_key}')\n",
    "\n",
    "# # D√©finition des dates dynamiques\n",
    "# today = datetime.now().strftime(\"%Y-%m-%d\")  # Aujourd'hui\n",
    "# last_month = (datetime.now() - timedelta(days=31)).strftime(\"%Y-%m-%d\")  # 31 jours avant aujourd'hui\n",
    "\n",
    "# # V√©rifier si lat et lon ont √©t√© correctement d√©finis avant de continuer\n",
    "# if lat is not None and lon is not None:\n",
    "#     # Construction de l'URL\n",
    "#     url_last_month = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat},{lon}/{last_month}/{today}?unitGroup=metric&key={vc_api_key}&contentType=json\"\n",
    "    \n",
    "#     # Tentative de r√©cup√©ration des donn√©es m√©t√©o\n",
    "#     try:\n",
    "#         response = requests.get(url_last_month)\n",
    "        \n",
    "#         # V√©rifier si la requ√™te a r√©ussi (code 200)\n",
    "#         if response.status_code == 200:\n",
    "#             vc_meteo_data = response.json()  # Essayer de d√©coder le JSON\n",
    "#             print(f\"Donn√©es m√©t√©o r√©cup√©r√©es : {vc_meteo_data}\")\n",
    "#         else:\n",
    "#             print(f\"Erreur lors de l'appel √† l'API, code de statut : {response.status_code}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de la r√©cup√©ration des donn√©es m√©t√©o : {e}\")\n",
    "# else:\n",
    "#     print(\"Les coordonn√©es (lat, lon) ne sont pas d√©finies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming the df_cleaned DataFrame already exists and contains the required data\n",
    "\n",
    "# # First, load your Visual Crossing Weather Data (example, you may already have it)\n",
    "# # Assuming vc_meteo_data is the JSON response from Visual Crossing\n",
    "# # Example of flattening the JSON\n",
    "# df_vc_meteo = pd.json_normalize(vc_meteo_data, record_path=[\"days\", \"hours\"], meta=[\"days\"])\n",
    "\n",
    "# # Convert the datetimeEpoch from Visual Crossing Weather data into Date and Hour columns\n",
    "# df_vc_meteo[\"Date\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d\")\n",
    "# df_vc_meteo[\"Hour\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%H\")\n",
    "\n",
    "# # Filter data from df_vc_meteo for the last 30 days\n",
    "# today = datetime.now()\n",
    "# thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "# today_str = today.strftime(\"%Y-%m-%d\")\n",
    "# thirty_days_ago_str = thirty_days_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# # Filter df_vc_meteo for the last 30 days\n",
    "# df_test_last_month = df_vc_meteo[['Date', 'Hour', 'windspeed']]\n",
    "# df_test_last_month = df_test_last_month[(df_test_last_month['Date'] >= thirty_days_ago_str) & \n",
    "#                                         (df_test_last_month['Date'] <= today_str)]\n",
    "\n",
    "# # Prepare df_cleaned for merging (add Date and Hour columns)\n",
    "# df_cleaned['Date'] = df_cleaned['Datetime'].dt.strftime(\"%Y-%m-%d\")\n",
    "# df_cleaned['Hour'] = df_cleaned['Datetime'].dt.strftime(\"%H\")\n",
    "\n",
    "# # Filter df_cleaned for the last 30 days\n",
    "# df_cleaned_last_month = df_cleaned[(df_cleaned['Date'] >= thirty_days_ago_str) & \n",
    "#                                    (df_cleaned['Date'] <= today_str)]\n",
    "\n",
    "# # Merge df_vc_meteo and df_cleaned based on Date and Hour\n",
    "# df_merged = df_test_last_month.merge(df_cleaned_last_month[['Date', 'Hour', 'Wind Speed (km/h)', 'wind_speed_10m']], \n",
    "#                                     on=['Date', 'Hour'], \n",
    "#                                     how='inner')\n",
    "\n",
    "# # Display the merged dataframe\n",
    "# print(df_merged.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_rename={'temperature_2m': 'T¬∞(C¬∞)',  'relative_humidity_2m': 'Relative Humidity (%)',\n",
    " 'dew_point_2m': 'Dew Point (¬∞C)', 'precipitation': 'Precipitation (mm)',  'pressure_msl':' Sea Level Pressure (hPa)', \n",
    " 'cloud_cover_low':'Low Clouds (%)', 'cloud_cover_mid' : 'Middle Clouds (%)',\t 'cloud_cover_high' : 'High Clouds (%)', \n",
    " 'visibility' : ' Visibility (%)',  'wind_direction': 'Wind Direction (¬∞)',\n",
    " 'wind_speed': 'Wind Speed (km/h)','wind_gust': 'Wind Gusts (km/h)', 'wave_height': 'Wave Height (m)',  'average_wave_period': 'Average Wave Period (s)',\n",
    " 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)','pressure': 'Pressure (hPA)',\n",
    " 'air_temperature': 'Air T¬∞','water_temperature': 'Water T¬∞'}\n",
    "\n",
    "df_cleaned = rename_columns(df_cleaned, col_to_rename)\n",
    "df_cleaned = drop_columns_if_exist(df_cleaned,['soil_temperature_0cm','rain', 'showers', 'is_day', 'id_x', 'id_y','soil_moisture_0_to_1cm'])\n",
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  R√©cup√©rer les donn√©es de l'API\n",
    "# vc_meteo_data = response.json()\n",
    "# print(vc_meteo_data)  # V√©rifiez les donn√©es r√©cup√©r√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normaliser les donn√©es JSON en DataFrame\n",
    "# df_vc_meteo = pd.json_normalize(vc_meteo_data, record_path=[\"days\", \"hours\"], meta=[\"days\"])\n",
    "\n",
    "# # Afficher la premi√®re ligne des donn√©es\n",
    "# df_vc_meteo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du timestamp en datetime\n",
    "df_vc_meteo[\"Date\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d\")\n",
    "df_vc_meteo[\"Hour\"] = pd.to_datetime(df_vc_meteo[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir les dates de filtrage pour les 30 derniers jours\n",
    "today = datetime.now()\n",
    "thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "# Convertir les dates en format YYYY-MM-DD\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "thirty_days_ago_str = thirty_days_ago.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les donn√©es des 30 derniers jours de df_vc_meteo\n",
    "df_test_last_month = df_vc_meteo[['Date', 'Hour', 'windspeed']]\n",
    "df_test_last_month = df_test_last_month[(df_test_last_month['Date'] >= thirty_days_ago_str) & \n",
    "                                        (df_test_last_month['Date'] <= today_str)]\n",
    "\n",
    "# Ajouter les colonnes Date et Hour √† df_42058\n",
    "df_42058.loc[:, 'Date'] = df_42058['Datetime'].dt.strftime(\"%Y-%m-%d\")\n",
    "df_42058.loc[:, 'Hour'] = df_42058['Datetime'].dt.strftime(\"%H\")\n",
    "\n",
    "# Filtrer les donn√©es des 30 derniers jours dans df_42058\n",
    "df_42058_last_month = df_42058[(df_42058['Date'] >= thirty_days_ago_str) & \n",
    "                                (df_42058['Date'] <= today_str)]\n",
    "\n",
    "# Fusionner les deux DataFrames sur Date et Hour\n",
    "df_test_merged = df_test_last_month.merge(df_42058_last_month[['Date', 'Hour', 'Wind Speed (km/h)', 'wind_speed_10m']], \n",
    "                                     on=['Date', 'Hour'], \n",
    "                                     how='inner')\n",
    "\n",
    "df_test_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def handle_null_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     row_count = df.shape[0]\n",
    "    \n",
    "#     # Initialisation des listes pour suivre les colonnes supprim√©es\n",
    "#     removed_columns = []\n",
    "#     non_numeric_columns_to_drop = []\n",
    "    \n",
    "#     # Utiliser lambda et apply() pour calculer le nombre de valeurs nulles dans chaque colonne\n",
    "#     null_counts = df.apply(lambda col: int(col.isnull().sum()))  # Calculer le nombre de NaN par colonne\n",
    "    \n",
    "#     # Condition : 1. Colonnes avec toutes les valeurs nulles ou 2. Plus de 50% de valeurs nulles et colonne non num√©rique\n",
    "#     columns_to_drop = null_counts[\n",
    "#         (null_counts == row_count) | \n",
    "#         ((null_counts > row_count * 0.5) & ~df.apply(lambda col: pd.api.types.is_numeric_dtype(col)))\n",
    "#     ].index\n",
    "    \n",
    "#     # Ajouter les noms des colonnes supprim√©es dans les listes appropri√©es\n",
    "#     for col in columns_to_drop:\n",
    "#         if null_counts[col] == row_count:\n",
    "#             removed_columns.append(col)  # Colonnes enti√®rement vides\n",
    "#         elif null_counts[col] > row_count * 0.5 and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             non_numeric_columns_to_drop.append(col)  # Colonnes > 50% nulles et non num√©riques\n",
    "    \n",
    "#     # Supprimer les colonnes identifi√©es\n",
    "#     df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "#     # Afficher les r√©sultats\n",
    "#     print(\"Colonnes supprim√©es pour avoir toutes les valeurs nulles:\")\n",
    "#     print(removed_columns)\n",
    "    \n",
    "#     print(\"\\nColonnes supprim√©es pour avoir plus de 50% de valeurs nulles et √™tre non num√©riques:\")\n",
    "#     print(non_numeric_columns_to_drop)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# # df_final = pd.read_csv('ton_fichier.csv') # Assure-toi que df_final est bien un DataFrame valide avant d'appeler la fonction\n",
    "# df_final = handle_null_values(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df_final.round(2)\n",
    "# print(df_final.columns)\n",
    "# df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explore_dict_keys(d, parent_key='', sep='_'):\n",
    "#     \"\"\"\n",
    "#     Explore un dictionnaire r√©cursivement pour obtenir toutes les cl√©s, y compris les sous-cl√©s,\n",
    "#     mais ne retourne pas les valeurs finales.\n",
    "\n",
    "#     :param d: Le dictionnaire √† explorer\n",
    "#     :param parent_key: La cl√© parent qui est utilis√©e pour concat√©ner les sous-cl√©s\n",
    "#     :param sep: Le s√©parateur utilis√© pour concat√©ner les cl√©s (par d√©faut '_')\n",
    "#     :return: Une liste des cl√©s (et sous-cl√©s)\n",
    "#     \"\"\"\n",
    "#     keys = []\n",
    "#     for k, v in d.items():\n",
    "#         new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "#         if isinstance(v, dict):  # Si la valeur est un dictionnaire, on explore r√©cursivement\n",
    "#             keys.append(new_key)  # Ajouter la cl√©, mais ne pas inclure la valeur\n",
    "#             keys.extend(explore_dict_keys(v, new_key, sep=sep))  # Continuer l'exploration\n",
    "#         else:\n",
    "#             keys.append(new_key)  # Ajouter la cl√© finale\n",
    "#     return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_key_path(d, target_key, path=[]):\n",
    "#     \"\"\"\n",
    "#     Recherche r√©cursive d'une cl√© dans un dictionnaire et retourne son chemin.\n",
    "#     :param d: dictionnaire\n",
    "#     :param target_key: cl√© recherch√©e\n",
    "#     :param path: liste pour stocker le chemin jusqu'√† la cl√©\n",
    "#     :return: chemin sous forme de liste\n",
    "#     \"\"\"\n",
    "#     if isinstance(d, dict):  # Si le dictionnaire est encore imbriqu√©\n",
    "#         for key, value in d.items():\n",
    "#             new_path = path + [key]\n",
    "#             if key == target_key:\n",
    "#                 return new_path\n",
    "#             elif isinstance(value, dict):\n",
    "#                 result = find_key_path(value, target_key, new_path)\n",
    "#                 if result:  # Si la cl√© est trouv√©e, retourner le chemin\n",
    "#                     return result\n",
    "#     return None  # Retourne None si la cl√© n'a pas √©t√© trouv√©e\n",
    "\n",
    "\n",
    "\n",
    "# # Recherche du chemin pour la cl√© 'marine_data'\n",
    "# path = find_key_path(table_dict, \"Marine Dataframe\")\n",
    "# print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto_convert Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (buoy_id, tables) in enumerate(table_dict.items()):  # Utilisation de .items() pour obtenir (cl√©, valeur)\n",
    "#     if isinstance(tables, dict):\n",
    "#         if idx == 1:  # V√©rifier si l'index est √©gal √† 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Rows of all Dataframes in total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
