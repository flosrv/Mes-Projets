{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from functions import *\n",
    "from IPython.core.display import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_postgresql_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\postgresql_creds.json\"\n",
    "with open(path_postgresql_creds, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    user = content[\"user\"]\n",
    "    password = content[\"password\"]\n",
    "    host = content[\"host\"]\n",
    "    port = content[\"port\"]\n",
    "\n",
    "db = \"MyProjects\"\n",
    "schema = \"End_To_End_Oceanography_ML\"\n",
    "\n",
    "# Créer l'engine PostgreSQL\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Available Stations ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stations and some metadata as a Pandas DataFrame\n",
    "stations_df = api.stations()\n",
    "# parse the response as a dictionary\n",
    "stations_df = api.stations(as_df=True)\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Dysfunctional Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [\"Failure\", \"ceased\",\"failed\",\"recovered\",\"stopped\", 'adrift']\n",
    "stations_id_set = set()\n",
    "\n",
    "print(f'Avant Filtre: {stations_df.shape[0]}')\n",
    "# Liste pour collecter les indices à supprimer\n",
    "indices_a_supprimer = []\n",
    "\n",
    "for idx, row in stations_df.iterrows():\n",
    "    station_id = row[\"Station\"]\n",
    "    \n",
    "    # Vérifier si \"Remark\" n'est pas NaN et si un des éléments de blacklist est dans \"Remark\"\n",
    "    if isinstance(row[\"Remark\"], str) and any(blacklist_word.lower() in row[\"Remark\"].lower() for blacklist_word in blacklist):\n",
    "        # Ajouter l'index à la liste\n",
    "        print(f'Station \"{station_id}\" Failed the Remarks test\\n')\n",
    "        indices_a_supprimer.append(idx)\n",
    "    else:\n",
    "        # Si l'URL en temps réel passe, afficher et ajouter le station_id à l'ensemble\n",
    "        print(f'Buoy {station_id} passed the Remark Test !')\n",
    "        stations_id_set.add(station_id)\n",
    "    \n",
    "# Supprimer les lignes après la boucle\n",
    "stations_df.drop(index=indices_a_supprimer, inplace=True)\n",
    "\n",
    "print(f'Après Filtre: {stations_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Table Names and Collect their Data in a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_id_list = list(stations_id_set)\n",
    "len(stations_id_list)\n",
    "one_station = stations_id_list[0]\n",
    "metadata = get_station_metadata(one_station)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_buoy, lon_buoy, station_name, station_id, station_zone, marine_data_table_name = parse_buoy_json(metadata)\n",
    "print(f\" {lat_buoy}, {lon_buoy}\\n{station_name}, {station_id}\\n{station_zone}, {marine_data_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in stations_df.iterrows():\n",
    "    stat_id = row[\"Station\"]\n",
    "    stations_id_list.append(stat_id)\n",
    "\n",
    "chosen_stations_id_set = set()\n",
    "station_table_mapping = {}  # Associe chaque station_id à un unique table_name\n",
    "table_name_set = set()\n",
    "\n",
    "for _ in range(200):\n",
    "    random_station = random.choice(stations_id_list)\n",
    "\n",
    "    if random_station not in chosen_stations_id_set:\n",
    "        chosen_stations_id_set.add(random_station)\n",
    "\n",
    "        buoy_metadata = get_station_metadata(random_station)\n",
    "        lat_buoy, lon_buoy, station_name, station_id,station_zone, table_name = parse_buoy_json(buoy_metadata)\n",
    "\n",
    "        # Vérifier si la station a déjà été ajoutée avec un nom de table\n",
    "        if random_station not in station_table_mapping:\n",
    "            station_table_mapping[random_station] = {\n",
    "                \"Name\": station_name,\n",
    "                \"lat\": lat_buoy,\n",
    "                \"lon\": lon_buoy,\n",
    "                \"zone\": station_zone,\n",
    "                \"table name\": table_name\n",
    "            }\n",
    "\n",
    "            table_name_set.add(table_name)\n",
    "\n",
    "# Affichage des résultats\n",
    "for item in table_name_set:\n",
    "    print(item)\n",
    "\n",
    "print(f'Longueur du set : {len(table_name_set)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_flush(message):\n",
    "    sys.stdout.write(f'\\r{message}  ')  # \\r permet de revenir au début de la ligne\n",
    "    sys.stdout.flush()  # Force l'affichage immédiat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Bronze Layer Table Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_tables_list = []\n",
    "\n",
    "for item in station_table_mapping.values():\n",
    "    bronze_table_name = f'bronze_{item[\"table name\"]}'\n",
    "    bronze_tables_list.append(bronze_table_name)\n",
    "bronze_tables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(station_table_mapping))\n",
    "print(first_key)\n",
    "\n",
    "buoy_data = NDBC.realtime_observations(first_key)\n",
    "df = pd.DataFrame(data=buoy_data)\n",
    "col =list(df.columns)\n",
    "col\n",
    "\n",
    "for table in bronze_tables_list:\n",
    "\n",
    "    try:\n",
    "        # Créer la table en appelant la fonction avec les bons paramètres\n",
    "        create_schema_and_table(conn=conn, schema=schema, table_name=table, col=col)\n",
    "        \n",
    "        print(f\"Table {table} created successfully!\")\n",
    "    except Exception as e:\n",
    "        # Capturer l'exception et afficher un message d'erreur avec l'exception\n",
    "        print(f\"Table {table} couldn't be created! \\nError: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_id_list =list(stations_id_set)\n",
    "buoy_chosen = random.choice(stations_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoy_chosen_metadata = get_station_metadata(buoy_chosen)\n",
    "buoy_chosen_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation avec un dictionnaire 'buoy_chosen_metadata'\n",
    "try:\n",
    "    lat_buoy, lon_buoy, station_name, station_id, station_zone, marine_data_table_name = parse_buoy_json(buoy_chosen_metadata)\n",
    "    print(lat_buoy, lon_buoy, station_name, station_id, station_zone, marine_data_table_name)\n",
    "except ValueError as e:\n",
    "    print(f\"Erreur lors du traitement des données: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine = NDBC.realtime_observations(buoy_chosen)\n",
    "print(type(buoy_chosen_metadata))\n",
    "df_marine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marine API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data From Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Remplacer `{station_id}` par l'identifiant de la station spécifique\n",
    "url = f\"https://www.ndbc.noaa.gov/station_page.php?station={station_id}\"\n",
    "\n",
    "# Faire une requête GET pour obtenir le HTML de la page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Parse le HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Trouver la division avec l'ID 'stationmetadata'\n",
    "    station_metadata = soup.find(id=\"stationmetadata\")\n",
    "    \n",
    "    # Vérifier si la division existe\n",
    "    if station_metadata:\n",
    "        # Chercher les deux images spécifiques\n",
    "        img_1 = station_metadata.find('img', src='/images/stations/3mfoam_scoop_mini.jpg')\n",
    "        img_2 = station_metadata.find('img', src='/images/buoycam/W64A_2025_03_15_1510.jpg')\n",
    "\n",
    "        # Si l'image 1 est trouvée, modifier son lien en absolu\n",
    "        if img_1:\n",
    "            img_1['src'] = urljoin(url, img_1['src'])\n",
    "\n",
    "        # Si l'image 2 est trouvée, modifier son lien en absolu\n",
    "        if img_2:\n",
    "            img_2['src'] = urljoin(url, img_2['src'])\n",
    "        \n",
    "        # Afficher directement le HTML avec les liens des images mis à jour\n",
    "        display(HTML(str(station_metadata)))  # Affiche la division en HTML rendu\n",
    "    else:\n",
    "        print(\"La division avec l'ID 'stationmetadata' n'a pas été trouvée.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la récupération de la page, statut: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_buoy_1 = (lat_buoy, lon_buoy)\n",
    "\n",
    "\n",
    "map = folium.Map(location=[lat_buoy,lon_buoy], zoom_start=6)\n",
    "\n",
    "folium.Marker(location=[lat_buoy, lon_buoy], popup=f\"Chosen Buoy : {station_name}, lat :{lat_buoy},lon :{lon_buoy}\").add_to(map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Stations from Caribbean Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prise en charge de plusieurs buoys à la fois\n",
    "caribbean_df = stations_df[(stations_df['Lat'] >= 9) & \n",
    "                           (stations_df['Lat'] <= 25) & \n",
    "                           (stations_df['Lon'] >= -85) & \n",
    "                           (stations_df['Lon'] <= -60)]\n",
    "caribbean_df.shape[0]\n",
    "\n",
    "buoys_ids = []\n",
    "\n",
    "for index, row in caribbean_df.iterrows():\n",
    "    buoys_ids.append(row['Station'])  \n",
    "print(len(buoys_ids))    \n",
    "\n",
    "# Liste pour stocker les dataframes\n",
    "all_dataframes = []\n",
    "\n",
    "# Compteurs pour les bouées réussies et échouées\n",
    "successful_buoy = 0\n",
    "failed_buoy = 0\n",
    "\n",
    "# Parcours de chaque bouée\n",
    "for buoy_id in buoys_ids:\n",
    "    try:\n",
    "        # Récupère les observations en temps réel pour chaque bouée\n",
    "        df_caribbean_buoy = NDBC.realtime_observations(buoy_id)\n",
    "\n",
    "        # Ajoute le dataframe à la liste\n",
    "        all_dataframes.append(df_caribbean_buoy)\n",
    "        successful_buoy += 1\n",
    "\n",
    "        # Efface et met à jour la ligne des bouées réussies\n",
    "        print(f\"\\r✅ Bouées réussies : {successful_buoy}\", end='', flush=True)\n",
    "\n",
    "    except Exception:\n",
    "        # Incrémente le compteur des échouées pour toutes les erreurs\n",
    "        failed_buoy += 1\n",
    "\n",
    "        # Efface et met à jour la ligne des bouées échouées\n",
    "        print(f\"\\r❌ Bouées échouées : {failed_buoy}\", end='', flush=True)\n",
    "\n",
    "# Affichage final propre avec un saut de ligne\n",
    "print(\"\\n\")\n",
    "\n",
    "# Concaténation des dataframes si disponibles\n",
    "if all_dataframes:\n",
    "    final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"✔️ Concaténation terminée. Nombre de lignes : {final_df.shape[0]} \\n Nombre de colonnes : {final_df.shape[1]}\")\n",
    "    print(f\"✅ Bouées réussies : {successful_buoy}\", end='', flush=True)\n",
    "    print(f\"❌ Bouées échouées : {failed_buoy}\", end='', flush=True)\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Aucune donnée récupérée.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine = NDBC.realtime_observations(nearest)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(df_marine.shape)\n",
    "df_marine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_marine_data_table_name = f\"Bronze_marine_data_{station_name.replace(' ', '_')}_{station_zone.replace(' ', '_')}_{str(lat_buoy).replace('.', '-')}_{str(lon_buoy).replace('.', '-')}\"\n",
    "print(bronze_marine_data_table_name)\n",
    "load_data_in_table(db=db, schema=schema, table_name=bronze_marine_data_table_name, df=df_marine,conn=conn,key_column='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = [lat_buoy, lon_buoy]\n",
    "df_meteo = meteo_api_request(coordinates=coordinates)\n",
    "print(df_meteo.shape)\n",
    "df_meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du nom de la table\n",
    "bronze_meteo_data_table_name = f\"bronze_meteo_data_{station_name.replace(' ', '_')}_{station_zone.replace(' ', '_')}_{str(lat_buoy).replace('.', '-')}_{str(lon_buoy).replace('.', '-')}\"\n",
    "bronze_meteo_data_table_name = bronze_meteo_data_table_name.replace('.', '-')\n",
    "load_data_in_table(db=db, schema=schema, table_name=bronze_meteo_data_table_name, df=df_meteo,conn=conn,key_column='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les nouvelles clés au JSON\n",
    "buoy_near_SM[\"bronze_marine\"] = bronze_marine_data_table_name\n",
    "buoy_near_SM[\"bronze_meteo\"] = bronze_meteo_data_table_name\n",
    "buoy_near_SM[\"db\"] = db\n",
    "buoy_near_SM[\"schema\"] = schema\n",
    "\n",
    "# Enregistrer en écrasant le fichier s'il existe\n",
    "with open(\"buoy_near_SM.json\", \"w\") as f:\n",
    "    json.dump(buoy_near_SM, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"buoy_near_SM.json\", \"r\") as f:\n",
    "    buoy_near_SM = json.load(f)\n",
    "\n",
    "\n",
    "path_postgresql_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\postgresql_creds.json\"\n",
    "with open(path_postgresql_creds, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    user = content[\"user\"]\n",
    "    password = content[\"password\"]\n",
    "    host = content[\"host\"]\n",
    "    port = content[\"port\"]\n",
    "\n",
    "# Créer l'engine PostgreSQL\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")\n",
    "conn = engine.connect()\n",
    "\n",
    "db = buoy_near_SM[\"db\"]\n",
    "schema = buoy_near_SM[\"schema\"] \n",
    "\n",
    "bronze_marine_data_table_name = buoy_near_SM[\"bronze_marine\"] \n",
    "bronze_meteo_data_table_name = buoy_near_SM[\"bronze_meteo\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_marine_to_clean = fetch_table_data(conn=conn, schema=schema, table_name= bronze_marine_data_table_name)\n",
    "    df_meteo_to_clean = fetch_table_data(conn=conn, schema=schema, table_name= bronze_meteo_data_table_name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(df):\n",
    "    for col in df.columns:\n",
    "        # Essayer de convertir en datetime\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                # Si tu connais le format, tu peux spécifier ici, par exemple '%Y-%m-%d'\n",
    "                # Exemple de format: '2021-01-01' ou '01/01/2021'\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='raise')  # Converte en datetime\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Essayer de convertir en numérique\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise')  # Converte en numérique\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return df\n",
    "\n",
    "df_marine_to_clean = auto_convert(df_marine_to_clean)\n",
    "print(df_marine_to_clean.dtypes)\n",
    "df_marine_to_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_to_clean = handle_null_values(df_marine_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean = auto_convert(df_meteo_to_clean)\n",
    "df_meteo_to_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_to_clean = handle_null_values(df_marine_to_clean)\n",
    "df_meteo_to_clean = handle_null_values(df_meteo_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_marine_to_clean.columns)\n",
    "print(df_meteo_to_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open-meteo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo = drop_columns_if_exist(df_meteo,['rain', 'showers','soil_moisture_0_to_1cm', 'cloud_cover', 'soil_temperature_0cm',\t'soil_moisture_0_to_1cm', 'is_day'])\n",
    "df_meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo.rename(columns={'temperature_2m': 'T°(C°)', \n",
    "                         'relative_humidity_2m': 'Relative Humidity (%)',\n",
    "                         'dew_point_2m': 'Dew Point (°C)', \n",
    "                         'precipitation': 'Precipitation (mm)', \n",
    "                         'pressure_msl':' Sea Level Pressure (hPa)', \n",
    "                         'cloud_cover_low':'Low Clouds (%)',\n",
    "                         'cloud_cover_mid' : 'Middle Clouds (%)',\t\n",
    "                         'cloud_cover_high' : 'High Clouds (%)', \n",
    "                         'visibility' : ' Visibility (%)', \n",
    "                         'wind_speed_10m' : 'Wind Speed (km/h)'}, \n",
    "                         inplace=True)\n",
    "df_marine.rename(columns={\n",
    "    'wind_direction': 'Wind Direction (°)',\n",
    "    'wind_speed': 'Wind Speed (km/h)',\n",
    "    'wind_gust': 'Wind Gusts (km/h)',\n",
    "    'wave_height': 'Wave Height (m)',\n",
    "    'average_wave_period': 'Average Wave Period (s)',\n",
    "    'dominant_wave_direction': 'Dominant Wave Direction (°)',\n",
    "    'pressure': 'Pressure (hPA)',\n",
    "    'air_temperature': 'Air T°',\n",
    "    'water_temperature': 'Water T°'}, \n",
    "    inplace=True)\n",
    "\n",
    "print(df_meteo.columns)\n",
    "print(df_marine.columns)\n",
    "print(df_marine.shape)\n",
    "print(df_meteo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer la jointure interne sur la colonne 'time'\n",
    "df_merged = pd.merge(df_marine, df_meteo, on = 'Datetime', how='inner')\n",
    "\n",
    "# Afficher le résultat\n",
    "print(df_merged.shape)\n",
    "print(df_merged.dtypes)\n",
    "df_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "df_merged = add_daytime_and_month_column(df_merged,'Datetime')\n",
    "\n",
    "print(df_merged.columns)\n",
    "df_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Wind Speed (km/h)'] = (df_merged['Wind Speed (km/h)_x']+ df_merged['Wind Speed (km/h)_y'])/2\n",
    "df_merged = drop_columns_if_exist(df_merged, ['Wind Speed (km/h)_x', 'Wind Speed (km/h)_y', 'Wind Gusts (km/h)'])\n",
    "\n",
    "print(df_merged.columns)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connexion BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.oauth2 import service_account\n",
    "# from google.cloud import storage  # Exemple pour Google Cloud Storage\n",
    "# from google.cloud import bigquery\n",
    "# from google.cloud.exceptions import NotFound\n",
    "# import pandas as pd\n",
    "# import pyarrow\n",
    "\n",
    "# path_to_google_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\google_credentials.json\"\n",
    "\n",
    "# bq_client = bigquery.Client.from_service_account_json(\n",
    "#     path_to_google_creds)\n",
    "# project_id = \"rare-bloom-419220\"\n",
    "# dataset_End_To_End_Oceanography_ML = \"End_To_End_Oceanography_ML\"\n",
    "# bq_client\n",
    "# # dataset_ref = bq_client.dataset('my_dataset_name', project=project_id)\n",
    "\n",
    "\n",
    "# # LIST DATASETS AND FIND ONE\n",
    "# datasets = list(bq_client.list_datasets())  # Make an API request.\n",
    "# project = client.project\n",
    "# bq_datasets_list =[]\n",
    "\n",
    "# if datasets:\n",
    "#     print(\"Datasets in project {}:\".format(project))\n",
    "#     for dataset in datasets:\n",
    "#         print(\"\\t{}\".format(dataset.dataset_id))\n",
    "#         bq_datasets_list.append(dataset.dataset_id)\n",
    "#     if dataset_End_To_End_Oceanography_ML in bq_datasets_list:\n",
    "#         dataset =  dataset_End_To_End_Oceanography_ML\n",
    "#         print(\"Dataset Found !\")\n",
    "# else:\n",
    "#     print(\"{} project does not contain any datasets.\".format(project))\n",
    "\n",
    "# # (developer): Set table_id to the ID of the table to determine existence.\n",
    "# # table_id = \"your-project.your_dataset.your_table\"\n",
    "\n",
    "# try:\n",
    "#     table_ref = bq_client.dataset(dataset).table(table_name)\n",
    "#     bq_client.get_table(table_ref)  # Make an API request.\n",
    "#     print(\"Table {} already exists.\".format(table_name))\n",
    "# except NotFound:\n",
    "#     print(\"Table {} is not found.\".format(table_name))\n",
    "\n",
    "\n",
    "# def clean_column_names(df):\n",
    "\n",
    "#     cleaned_columns = []\n",
    "#     for column in df.columns:\n",
    "#         # Remplacer tous les caractères non alphanumériques (sauf underscores) par un underscore\n",
    "#         cleaned_column = re.sub(r'[^A-Za-z0-9_]', '_', column)\n",
    "        \n",
    "#         # Ajouter le nom de colonne nettoyé à la liste\n",
    "#         cleaned_columns.append(cleaned_column)\n",
    "    \n",
    "#     # Appliquer les nouveaux noms de colonnes au DataFrame\n",
    "#     df.columns = cleaned_columns\n",
    "#     return df\n",
    "\n",
    "# df_merged = clean_column_names(df_merged)\n",
    "\n",
    "# table_id = f\"{project_id}.{dataset}.{table_name}\"\n",
    "\n",
    "\n",
    "# try:\n",
    "#     bq_client.get_table(table_id)  # Make an API request.\n",
    "#     print(\"Table {} already exists.\".format(table_id))\n",
    "# except NotFound:\n",
    "#     print(\"Table {} is not found.\".format(table_id))\n",
    "#     bq_client.create_table(table_id)\n",
    "#     print(\"Creation of the Table {}.\".format(table_id))\n",
    "\n",
    "\n",
    "# def load_data_to_bigquery(client, dataset: str = None, table: str = None, df: pd.DataFrame = None, key_column: str = 'Datetime', table_id: str = None):\n",
    "   \n",
    "    \n",
    "#     # Fonction pour détecter et convertir les types de données\n",
    "#     def convert_column_types(df):\n",
    "#         for column in df.columns:\n",
    "#             dtype = df[column].dtype\n",
    "\n",
    "#             if dtype == 'object':  # Chaînes de caractères\n",
    "#                 df[column] = df[column].astype(str)\n",
    "#             elif dtype == 'datetime64[ns]':  # Datetime\n",
    "#                 df[column] = pd.to_datetime(df[column], errors='coerce').dt.tz_localize('UTC', ambiguous='NaT').dt.tz_localize(None)\n",
    "#             elif dtype == 'float64':  # Float\n",
    "#                 df[column] = df[column].astype('float32')\n",
    "#             elif dtype == 'int64':  # Integer\n",
    "#                 df[column] = df[column].astype('int64')  # On garde int64, car BigQuery supporte ce type\n",
    "#             else:\n",
    "#                 # Autres types, on les convertit en string\n",
    "#                 df[column] = df[column].astype(str)\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     # Convertir les types des colonnes\n",
    "#     df = convert_column_types(df)\n",
    "\n",
    "#     if table_id:\n",
    "#         # Si table_id est fourni, on l'utilise directement.\n",
    "#         full_table_id = table_id\n",
    "#     elif dataset and table:\n",
    "#         # Si table_id n'est pas fourni, on construit table_id à partir de dataset et table.\n",
    "#         full_table_id = f\"{client.project}.{dataset}.{table}\"\n",
    "#     else:\n",
    "#         raise ValueError(\"Il faut fournir soit 'table_id' ou les paramètres 'dataset' et 'table' séparés.\")\n",
    "\n",
    "#     # Vérifier si le dataset existe, sinon le créer\n",
    "#     try:\n",
    "#         dataset = full_table_id.split('.')[1]\n",
    "#         client.get_dataset(dataset)  # Vérifie si le dataset existe\n",
    "#         print(f\"Le dataset {dataset} existe déjà.\")\n",
    "#     except NotFound:\n",
    "#         print(f\"Le dataset {dataset} n'existe pas. Création du dataset...\")\n",
    "#         client.create_dataset(dataset)  # Crée le dataset s'il n'existe pas\n",
    "#         print(f\"Le dataset {dataset} a été créé.\")\n",
    "\n",
    "#     # Vérifier si la table existe, sinon la créer\n",
    "#     try:\n",
    "#         client.get_table(full_table_id)  # Vérifie si la table existe\n",
    "#         print(f\"La table {full_table_id} existe déjà.\")\n",
    "#     except NotFound:\n",
    "#         print(f\"La table {full_table_id} n'existe pas. Création de la table...\")\n",
    "#         # Créer la table avec le schéma du DataFrame\n",
    "#         schema = []\n",
    "#         for name, dtype in df.dtypes.items():\n",
    "#             if name == 'Datetime':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.TIMESTAMP))\n",
    "#             elif dtype == 'float32' or dtype == 'float64':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.FLOAT64))\n",
    "#             elif dtype == 'int64':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.INTEGER))\n",
    "#             else:\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING))\n",
    "        \n",
    "#         # Créer la table avec le schéma\n",
    "#         table = bigquery.Table(full_table_id, schema=schema)\n",
    "#         client.create_table(table)  # Crée la table si elle n'existe pas\n",
    "#         print(f\"La table {full_table_id} a été créée.\")\n",
    "\n",
    "#     # Préparer les données pour l'insertion\n",
    "#     if key_column in df.columns:\n",
    "#         # Si la colonne clé est fournie, supprimer les doublons en fonction de cette colonne\n",
    "#         df = df.drop_duplicates(subset=[key_column])\n",
    "\n",
    "#     # Charger les données dans la table BigQuery\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         schema=[\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.TIMESTAMP) if name == 'Datetime' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.FLOAT64) if dtype == 'float32' or dtype == 'float64' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.INTEGER) if dtype == 'int64' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING)\n",
    "#             for name, dtype in df.dtypes.items()\n",
    "#         ],\n",
    "#         write_disposition=\"WRITE_APPEND\"  # Ajoute les données sans écraser les anciennes\n",
    "#     )\n",
    "\n",
    "#     # Charger le DataFrame dans BigQuery\n",
    "#     job = client.load_table_from_dataframe(df, full_table_id, job_config=job_config)\n",
    "#     job.result()  # Attendre la fin de la tâche\n",
    "#     print(f\"Données chargées dans la table {full_table_id}.\")\n",
    "\n",
    "# # Exemple d'appel à la fonction\n",
    "# load_data_to_bigquery(table_id=table_id, client=bq_client, df=df_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
