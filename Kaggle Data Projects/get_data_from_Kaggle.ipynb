{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q kaggle\n",
    "%pip install -q kagglehub\n",
    "%pip install -q dask\n",
    "%pip install -q ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, requests\n",
    "import kagglehub, re, json, os\n",
    "import requests, subprocess\n",
    "import sys, time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentification r√©ussie avec le fichier 'C:\\Credentials\\kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# D√©finis le chemin du fichier kaggle.json\n",
    "path = r\"C:\\Credentials\\kaggle.json\"\n",
    "#json file : {\"username\":\"flosrv\",\"api_key\":\"44c79a4d5d1534092d24ee29f34e6ee3\"}\n",
    "# Charge les credentials depuis le fichier JSON\n",
    "with open(path, 'r') as file:\n",
    "    content = json.load(file)\n",
    "\n",
    "# R√©cup√®re les valeurs du fichier\n",
    "username = content[\"username\"]\n",
    "api_key = content[\"key\"]  # ‚ö†Ô∏è Assure-toi que la cl√© s'appelle bien \"key\" dans ton fichier JSON\n",
    "\n",
    "# ‚úÖ D√©finis les variables d'environnement avant de les utiliser\n",
    "os.environ['KAGGLE_USERNAME'] = username\n",
    "os.environ['KAGGLE_KEY'] = api_key\n",
    "import kaggle\n",
    "from kaggle import KaggleApi\n",
    "from kaggle import api # import the already authenticated API client\n",
    "# 1. **Authentification avec Kaggle**\n",
    "def authenticate_kaggle():\n",
    "    # V√©rifie si le fichier kaggle.json est dans le bon dossier\n",
    "    kaggle_json_path = os.path.expanduser(path)  # pour Linux/macOS\n",
    "    # Pour Windows, utilisez : os.path.expanduser(r'C:\\Users\\<Votre-Nom>\\.kaggle\\kaggle.json')\n",
    "\n",
    "    # Si le fichier n'existe pas, avertir l'utilisateur\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"Le fichier 'kaggle.json' n'est pas trouv√©. T√©l√©chargez-le depuis votre compte Kaggle.\")\n",
    "        print(\"Assurez-vous de le placer dans le r√©pertoire ~/.kaggle/ (Linux/macOS) ou C:\\\\Users\\\\<Your-Username>\\\\.kaggle\\\\ (Windows).\")\n",
    "        return False  # L'authentification √©choue\n",
    "    else:\n",
    "        print(f\"Authentification r√©ussie avec le fichier '{kaggle_json_path}'\")\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()  # Authentification avec le fichier 'kaggle.json'\n",
    "        return True\n",
    "\n",
    "try:\n",
    "    # Ex√©cuter l'authentification\n",
    "    authenticate_kaggle()\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'authentification : {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\f.gionnane\\Downloads\\Mes-Projets\\Kaggle Data Projects\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lister les datasets correspondant √† '['sales', 'marketing', 'customer', 'finance', 'supply chain', 'human resources', 'retail', 'ecommerce', 'operations', 'real estate']' :\n",
      "\n",
      "39 datasets trouv√©s.\n"
     ]
    }
   ],
   "source": [
    "def list_datasets(keyword=None, usability_min=None, min_mb=None, columns=None):\n",
    "    print(f\"Lister les datasets correspondant √† '{keyword if keyword else 'tous les datasets'}' :\")\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset metadata:\n",
    "    'id', 'ref', 'subtitle', 'creatorName', 'creatorUrl', 'totalBytes',\n",
    "        'url', 'lastUpdated', 'downloadCount', 'licenseName', 'description',\n",
    "        'ownerName', 'ownerRef', 'kernelCount', 'title', 'viewCount',\n",
    "        'voteCount', 'currentVersionNumber', 'usabilityRating', 'tags'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Traitement des mots-cl√©s\n",
    "    if keyword:\n",
    "        if isinstance(keyword, str):\n",
    "            keyword = [keyword]\n",
    "        keyword = [re.escape(kw) for kw in keyword]\n",
    "\n",
    "    # Recherche des datasets via l'API Kaggle\n",
    "    datasets = []\n",
    "    for kw in keyword:\n",
    "        datasets.extend(api.dataset_list(search=kw))\n",
    "\n",
    "    result = []\n",
    "    for dataset in datasets:\n",
    "        dataset_dict = dataset.to_dict()  # Convertir le dataset en dictionnaire\n",
    "        \n",
    "        title = dataset_dict.get(\"title\", \"\")\n",
    "        ref = dataset_dict.get(\"ref\", \"\")\n",
    "        description = dataset_dict.get(\"description\", \"\")\n",
    "        lastUpdated = dataset_dict.get(\"lastUpdated\", \"\")\n",
    "        voteCount = dataset_dict.get(\"voteCount\", 0)\n",
    "        tags = dataset_dict.get(\"tags\", [])\n",
    "        usabilityRating = dataset_dict.get(\"usabilityRating\", 0)\n",
    "\n",
    "        # Convertir la taille en Mo\n",
    "        dataset_size = dataset_dict.get(\"totalBytes\", 0)\n",
    "        dataset_size_mb = dataset_size / (1024 * 1024)\n",
    "\n",
    "        # Filtrage selon la taille du dataset\n",
    "        if min_mb is not None and dataset_size_mb < min_mb:\n",
    "            continue\n",
    "\n",
    "        # Filtrage selon la note d'utilisabilit√©\n",
    "        if usability_min is not None and usabilityRating < usability_min:\n",
    "            continue\n",
    "\n",
    "        # Si des colonnes sp√©cifiques sont demand√©es\n",
    "        if columns is not None:\n",
    "            dataset_dict = {key: dataset_dict[key] for key in columns if key in dataset_dict}\n",
    "\n",
    "        # Ajouter les informations dans le r√©sultat\n",
    "        result_dict = {\n",
    "            \"title\": title,\n",
    "            \"ref\": ref,\n",
    "            \"description\": description,\n",
    "            \"lastUpdated\": lastUpdated,\n",
    "            \"voteCount\": voteCount,\n",
    "            \"tags\": tags,\n",
    "            \"usabilityRating\": usabilityRating,\n",
    "            \"dataset_size_mb\": round(dataset_size_mb,2)\n",
    "        }\n",
    "\n",
    "        result.append(result_dict)\n",
    "\n",
    "    # Cr√©ation du DataFrame\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    print(f\"\\n{df.shape[0]} datasets trouv√©s.\")\n",
    "    return df\n",
    "\n",
    "keywords = [\n",
    "    \"sales\",          # Ventes : analyses de performance commerciale\n",
    "    \"marketing\",      # Campagnes, ROI, funnel, etc.\n",
    "    \"customer\",       # Donn√©es clients : segmentation, churn, satisfaction\n",
    "    \"finance\",        # Budgets, pr√©visions, √©tats financiers\n",
    "    \"supply chain\",   # Logistique, stocks, pr√©vision de demande\n",
    "    \"human resources\",# RH : turnover, performance, recrutement\n",
    "    \"retail\",         # Analyse de produits, points de vente, inventaire\n",
    "    \"ecommerce\",      # Donn√©es web, comportement d'achat, panier moyen\n",
    "    \"operations\",     # Optimisation de processus internes\n",
    "    \"real estate\"     # Immobilier : investissements, pr√©visions de prix\n",
    "]\n",
    "\n",
    "def download_dataset(dataset_ref: str, path: str = \".\"):\n",
    "\n",
    "    \"\"\"\n",
    "    T√©l√©charge un dataset Kaggle via sa ref en cr√©ant un dossier pour chaque dataset, avec une barre de progression.\n",
    "\n",
    "    Args:\n",
    "        dataset_ref (str): R√©f√©rence du dataset sous la forme \"username/dataset-name\".\n",
    "        path (str, optionnel): R√©pertoire o√π enregistrer le fichier t√©l√©charg√©. Par d√©faut: \".\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    dataset_folder = os.path.join(\".\", dataset_ref.replace(\"/\", \"_\"))\n",
    "    if os.path.exists(dataset_folder):\n",
    "        print(f\"ÔøΩÔøΩ Le dataset '{dataset_ref}' est d√©j√† t√©l√©charg√© dans le r√©pertoire courant.\")\n",
    "        return\n",
    "    os.makedirs(dataset_folder)  # Cr√©er le dossier si il n'existe pas d√©j√†\n",
    "    print(f\"ÔøΩÔøΩ Cr√©ation du dossier '{dataset_folder}' pour le dataset '{dataset_ref}'...\")     \n",
    "\n",
    "    # R√©cup√©rer la taille du dataset via l'API Kaggle\n",
    "    metadata_url = f\"https://www.kaggle.com/api/v1/datasets/metadata/{dataset_ref}\"\n",
    "    try:\n",
    "        response = requests.get(metadata_url)\n",
    "        if response.status_code == 200:\n",
    "            metadata = response.json()\n",
    "            dataset_size = metadata[\"totalBytes\"] / 1_000_000  # Conversion en Mo\n",
    "            print(f\"üì• T√©l√©chargement de '{dataset_ref}' ({dataset_size:.2f} MB)...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Impossible de r√©cup√©rer la taille du dataset.\")\n",
    "            dataset_size = None\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des m√©tadonn√©es.\")\n",
    "        dataset_size = None\n",
    "\n",
    "    # Commande de t√©l√©chargement\n",
    "    command = [\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_ref, \"-p\", dataset_folder, \"--unzip\"]\n",
    "\n",
    "    try:\n",
    "        # D√©marrer le t√©l√©chargement avec suivi de progression\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Barre de progression\n",
    "        with tqdm(total=dataset_size, unit=\"MB\", unit_scale=True, desc=\"üì• Progression\") as pbar:\n",
    "            while process.poll() is None:\n",
    "                time.sleep(1)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # V√©rifier si le t√©l√©chargement a r√©ussi\n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ Dataset '{dataset_ref}' t√©l√©charg√© avec succ√®s dans '{dataset_folder}'.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur lors du t√©l√©chargement. Code de retour: {process.returncode}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Erreur dans la commande Kaggle. Code de retour: {e.returncode}\")\n",
    "        print(f\"Erreur : {e.stderr.decode('utf-8')}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Erreur : La commande 'kaggle' n'a pas √©t√© trouv√©e. Installez l'API Kaggle.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Une erreur inattendue s'est produite : {str(e)}\")\n",
    "\n",
    "df_consulting_datasets =list_datasets(keyword=keywords, usability_min=0.6, min_mb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ref</th>\n",
       "      <th>description</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>voteCount</th>\n",
       "      <th>tags</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>dataset_size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vehicle Sales Data</td>\n",
       "      <td>syedanwarafridi/vehicle-sales-data</td>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T20:16:17.790Z</td>\n",
       "      <td>584</td>\n",
       "      <td>[{'ref': 'transportation', 'name': 'transporta...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telecom customer</td>\n",
       "      <td>abhinav89/telecom-customer</td>\n",
       "      <td></td>\n",
       "      <td>2017-08-27T03:01:50.747Z</td>\n",
       "      <td>207</td>\n",
       "      <td>[{'ref': 'business', 'name': 'business', 'desc...</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>13.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer Support on Twitter</td>\n",
       "      <td>thoughtvector/customer-support-on-twitter</td>\n",
       "      <td></td>\n",
       "      <td>2017-12-03T23:44:27.157Z</td>\n",
       "      <td>495</td>\n",
       "      <td>[{'ref': 'business', 'name': 'business', 'desc...</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>168.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US Funds dataset from Yahoo Finance</td>\n",
       "      <td>stefanoleone992/mutual-funds-and-etfs</td>\n",
       "      <td></td>\n",
       "      <td>2021-12-11T16:06:22.453Z</td>\n",
       "      <td>253</td>\n",
       "      <td>[{'ref': 'united states', 'name': 'united stat...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>353.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Massive Yahoo Finance Dataset</td>\n",
       "      <td>iveeaten3223times/massive-yahoo-finance-dataset</td>\n",
       "      <td></td>\n",
       "      <td>2023-11-29T17:24:31.693Z</td>\n",
       "      <td>51</td>\n",
       "      <td>[{'ref': 'research', 'name': 'research', 'desc...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0                   Vehicle Sales Data   \n",
       "1                     Telecom customer   \n",
       "2          Customer Support on Twitter   \n",
       "3  US Funds dataset from Yahoo Finance   \n",
       "4        Massive Yahoo Finance Dataset   \n",
       "\n",
       "                                               ref description  \\\n",
       "0               syedanwarafridi/vehicle-sales-data               \n",
       "1                       abhinav89/telecom-customer               \n",
       "2        thoughtvector/customer-support-on-twitter               \n",
       "3            stefanoleone992/mutual-funds-and-etfs               \n",
       "4  iveeaten3223times/massive-yahoo-finance-dataset               \n",
       "\n",
       "                lastUpdated  voteCount  \\\n",
       "0  2024-02-21T20:16:17.790Z        584   \n",
       "1  2017-08-27T03:01:50.747Z        207   \n",
       "2  2017-12-03T23:44:27.157Z        495   \n",
       "3  2021-12-11T16:06:22.453Z        253   \n",
       "4  2023-11-29T17:24:31.693Z         51   \n",
       "\n",
       "                                                tags  usabilityRating  \\\n",
       "0  [{'ref': 'transportation', 'name': 'transporta...         1.000000   \n",
       "1  [{'ref': 'business', 'name': 'business', 'desc...         0.705882   \n",
       "2  [{'ref': 'business', 'name': 'business', 'desc...         0.911765   \n",
       "3  [{'ref': 'united states', 'name': 'united stat...         1.000000   \n",
       "4  [{'ref': 'research', 'name': 'research', 'desc...         1.000000   \n",
       "\n",
       "   dataset_size_mb  \n",
       "0            18.84  \n",
       "1            13.57  \n",
       "2           168.58  \n",
       "3           353.34  \n",
       "4            22.78  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consulting_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consulting_datasets[\"tags\"] = df_consulting_datasets[\"tags\"].apply(lambda tag_list: [tag.get(\"name\", \"\") for tag in tag_list if not isinstance(tag_list, list)])\n",
    "df_consulting_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['id',  'title', 'ref', 'totalBytes', 'usabilityRating', \n",
    "          'description', 'url', 'lastUpdated', 'topicCount', \n",
    "          'voteCount', 'currentVersionNumber', 'tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Kaggle Data Projects\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df_consulting_datasets.iterrows():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     url = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m     url_list.append(url)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# ref = row[\"ref\"]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Kaggle Data Projects\\env\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Kaggle Data Projects\\env\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\f.gionnane\\Documents\\Data Engineering\\Kaggle Data Projects\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'url'"
     ]
    }
   ],
   "source": [
    "url_list = []\n",
    "metadata_list = []\n",
    "count = 0\n",
    "\n",
    "download_path = \"/downloaded_datasets\"\n",
    "\n",
    "for idx, row in df_consulting_datasets.iterrows():\n",
    "    url = row[\"url\"]\n",
    "    url_list.append(url)\n",
    "    ref = row[\"ref\"]\n",
    "    try:\n",
    "        download_dataset(ref,path=download_path)\n",
    "        print(f\"Dataset '{ref}' t√©l√©charg√© avec succ√®s.\")\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset for {ref} (Error: {str(e)})\")\n",
    "        continue\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        content = response.json()  # Stocker le JSON\n",
    "        metadata_list.append(content)  # Ajouter √† la liste des m√©tadonn√©es\n",
    "        \n",
    "        # V√©rifier si le dataset est priv√©\n",
    "        if content[\"info\"][\"isPrivate\"]:\n",
    "            df_consulting_datasets.drop(labels=idx, inplace=True)  # Supprimer la ligne\n",
    "            count += 1\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error fetching metadata for {url} (Status code: {response.status_code})\")\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_list)\n",
    "print(f\"Nombre de datasets priv√©s supprim√©s : {count}\")\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser la colonne \"info\" avant d'appliquer le filtrage des colonnes\n",
    "df_normalized = df_metadata.join(pd.json_normalize(df_metadata[\"info\"]))\n",
    "df_normalized.drop(columns=[\"info\"], inplace=True)\n",
    "\n",
    "# Appliquer le filtrage des colonnes apr√®s la normalisation\n",
    "df_normalized = df_normalized.loc[:, ~df_normalized.columns.str.contains('has|nullable', case=False)]\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_normalized.shape)\n",
    "df_normalized[\"isPrivate\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(keyword):\n",
    "    print(f\"Lister les mod√®les correspondant √† '{keyword}' :\")\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    models = api.models_list(search=keyword)\n",
    "    desired_columns = [\n",
    "        'id', 'ref', 'title', 'description', 'url',\n",
    "        'totalVotes', 'lastUpdated', 'tags', 'licenseName'\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for model in models:\n",
    "        model_dict = {key: model[key] for key in model if key in desired_columns}\n",
    "        result.append(model_dict)\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    # Suppression des colonnes inutiles contenant \"has\" ou \"Nullable\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('has|Nullable', case=False)]\n",
    "    df[\"tags\"] = df[\"tags\"].apply(lambda x: [d[\"name\"] for d in x] if x is not None else [])\n",
    "    # V√©rification des liens URL pour voir si les mod√®les sont accessibles\n",
    "    for idx, row in df.iterrows():\n",
    "        url = row[\"url\"]\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code >= 400:\n",
    "                df.drop(idx, inplace=True)\n",
    "        except requests.exceptions.RequestException:\n",
    "            df.drop(idx, inplace=True)\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list =[]\n",
    "\n",
    "for idx, row in df_ocean_datasets.iterrows():\n",
    "    url = row[\"url\"]\n",
    "    url_list.append(url)\n",
    "    title = row[\"title\"]\n",
    "    if title == 'OCEAN DATA / CLIMATE CHANGE / NASA':\n",
    "        try:\n",
    "            download_dataset(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else :\n",
    "        print('dataset not found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
