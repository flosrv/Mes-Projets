{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q kaggle\n",
    "%pip install -q kagglehub\n",
    "%pip install -q dask\n",
    "%pip install -q ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, requests\n",
    "import kagglehub, re, json, os\n",
    "import requests, subprocess\n",
    "import sys, time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentification r√©ussie avec le fichier 'C:\\Credentials\\kaggle.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D√©finis le chemin du fichier kaggle.json\n",
    "path = r\"C:\\Credentials\\kaggle.json\"\n",
    "#json file : {\"username\":\"flosrv\",\"api_key\":\"44c79a4d5d1534092d24ee29f34e6ee3\"}\n",
    "# Charge les credentials depuis le fichier JSON\n",
    "with open(path, 'r') as file:\n",
    "    content = json.load(file)\n",
    "\n",
    "# R√©cup√®re les valeurs du fichier\n",
    "username = content[\"username\"]\n",
    "api_key = content[\"key\"]  # ‚ö†Ô∏è Assure-toi que la cl√© s'appelle bien \"key\" dans ton fichier JSON\n",
    "\n",
    "# ‚úÖ D√©finis les variables d'environnement avant de les utiliser\n",
    "os.environ['KAGGLE_USERNAME'] = username\n",
    "os.environ['KAGGLE_KEY'] = api_key\n",
    "import kaggle\n",
    "from kaggle import api # import the already authenticated API client\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# 1. **Authentification avec Kaggle**\n",
    "def authenticate_kaggle():\n",
    "    # V√©rifie si le fichier kaggle.json est dans le bon dossier\n",
    "    kaggle_json_path = os.path.expanduser(path)  # pour Linux/macOS\n",
    "    # Pour Windows, utilisez : os.path.expanduser(r'C:\\Users\\<Votre-Nom>\\.kaggle\\kaggle.json')\n",
    "\n",
    "    # Si le fichier n'existe pas, avertir l'utilisateur\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"Le fichier 'kaggle.json' n'est pas trouv√©. T√©l√©chargez-le depuis votre compte Kaggle.\")\n",
    "        print(\"Assurez-vous de le placer dans le r√©pertoire ~/.kaggle/ (Linux/macOS) ou C:\\\\Users\\\\<Your-Username>\\\\.kaggle\\\\ (Windows).\")\n",
    "        return False  # L'authentification √©choue\n",
    "    else:\n",
    "        print(f\"Authentification r√©ussie avec le fichier '{kaggle_json_path}'\")\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()  # Authentification avec le fichier 'kaggle.json'\n",
    "        return True\n",
    "\n",
    "# Ex√©cuter l'authentification\n",
    "authenticate_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\f.gionnane\\Downloads\\Mes-Projets\\Kaggle Data Projects\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datasets(keyword=None, usability_min=None, min_mb=None, columns=None):\n",
    "    print(f\"Lister les datasets correspondant √† '{keyword if keyword else 'tous les datasets'}' :\")\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    datasets = api.datasets_list(search=keyword) if keyword else api.datasets_list()\n",
    "    \n",
    "    result = []\n",
    "    for dataset in datasets:\n",
    "        dataset_dict = dataset  \n",
    "\n",
    "        if columns is not None:\n",
    "            dataset_dict = {key: dataset[key] for key in dataset if key in columns}\n",
    "\n",
    "        result.append(dataset_dict)\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, ~df.columns.str.contains('has|Nullable', case=False)]\n",
    "\n",
    "    if \"tags\" in df.columns:\n",
    "        df[\"tags\"] = df[\"tags\"].apply(\n",
    "            lambda x: [d[\"nameNullable\"] for d in x if isinstance(d, dict) and \"nameNullable\" in d] if isinstance(x, list) else []\n",
    "        )\n",
    "\n",
    "    if \"usabilityRating\" in df.columns:\n",
    "        df[\"usabilityRating\"] = round(df[\"usabilityRating\"], 2)\n",
    "\n",
    "    if usability_min is not None and \"usabilityRating\" in df.columns:\n",
    "        df = df[df[\"usabilityRating\"] >= usability_min]\n",
    "\n",
    "    if min_mb is not None and \"Size (Mb)\" in df.columns:\n",
    "        df = df[df[\"Size (Mb)\"] >= min_mb]\n",
    "\n",
    "    if keyword:\n",
    "        df = df[\n",
    "            df[\"title\"].str.contains(keyword, case=False, na=False) |  \n",
    "            df[\"tags\"].apply(lambda tags: any(keyword in tag.lower() for tag in tags))\n",
    "        ]\n",
    "\n",
    "    if \"url\" in df.columns:\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                response = requests.get(row[\"url\"])\n",
    "                if response.status_code >= 400:\n",
    "                    df.drop(idx, inplace=True)\n",
    "            except requests.exceptions.RequestException:\n",
    "                df.drop(idx, inplace=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ref = row[\"ref\"]\n",
    "        title = row[\"title\"]\n",
    "        \n",
    "        response = requests.get(f\"https://www.kaggle.com/api/v1/datasets/metadata/{ref}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json()  \n",
    "            \n",
    "            if content[\"info\"][\"isPrivate\"]:\n",
    "                df.drop(labels=idx, inplace=True)  \n",
    "        \n",
    "        else:\n",
    "            print(f\"Error fetching metadata for {title} (Status code: {response.status_code})\")\n",
    "\n",
    "    if columns is not None:\n",
    "        df = df[[col for col in columns if col in df.columns]]  # R√©organiser les colonnes dans l'ordre de la liste fournie\n",
    "\n",
    "    if \"totalBytes\" in df.columns:\n",
    "        df[\"totalBytes\"] = round(df[\"totalBytes\"] / 1_000_000, 2)\n",
    "        df.rename(columns={\"totalBytes\": \"Size (Mb)\"}, inplace=True)\n",
    "    \n",
    "    if \"usabilityRating\" in df.columns:\n",
    "        df.rename(columns={\"usabilityRating\": \"Usability\"}, inplace=True)\n",
    "    \n",
    "\n",
    "    print(f'{df.shape[0]} r√©sultat(s) trouv√©(s) ! ')\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_dataset(dataset_ref: str, path: str =path):\n",
    "    \"\"\"\n",
    "    T√©l√©charge un dataset Kaggle via l'API en cr√©ant un dossier pour chaque dataset, avec une barre de progression.\n",
    "\n",
    "    Args:\n",
    "        dataset_ref (str): R√©f√©rence du dataset sous la forme \"username/dataset-name\".\n",
    "        path (str, optionnel): R√©pertoire o√π enregistrer le fichier t√©l√©charg√©. Par d√©faut: \".\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dataset_identifier = dataset_ref\n",
    "    # Cr√©ation du dossier pour le dataset avec son nom\n",
    "    dataset_folder = os.path.join(path, dataset_identifier.replace(\"/\", \"_\"))\n",
    "\n",
    "    # Si le dossier existe d√©j√†, informer l'utilisateur\n",
    "    if os.path.exists(dataset_folder):\n",
    "        print(f\"‚úÖ Le dataset '{dataset_identifier}' est d√©j√† t√©l√©charg√© dans '{dataset_folder}'.\")\n",
    "        return\n",
    "\n",
    "    # Cr√©er un dossier pour le dataset\n",
    "    os.makedirs(dataset_folder)\n",
    "    print(f\"üìÇ Cr√©ation du dossier pour le dataset '{dataset_identifier}' dans '{dataset_folder}'...\")\n",
    "\n",
    "    # R√©cup√©rer la taille du dataset via l'API Kaggle\n",
    "    metadata_url = f\"https://www.kaggle.com/api/v1/datasets/metadata/{dataset_identifier}\"\n",
    "    try:\n",
    "        response = requests.get(metadata_url)\n",
    "        if response.status_code == 200:\n",
    "            metadata = response.json()\n",
    "            dataset_size = metadata[\"Size (Mb)\"] / 1_000_000  # Conversion en Mo\n",
    "            print(f\"üì• T√©l√©chargement de '{dataset_identifier}' ({dataset_size:.2f} MB)...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Impossible de r√©cup√©rer la taille du dataset.\")\n",
    "            dataset_size = None\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des m√©tadonn√©es.\")\n",
    "        dataset_size = None\n",
    "\n",
    "    # Commande de t√©l√©chargement\n",
    "    command = [\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_identifier, \"-p\", dataset_folder, \"--unzip\"]\n",
    "\n",
    "    try:\n",
    "        # D√©marrer le t√©l√©chargement avec suivi de progression\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Barre de progression\n",
    "        with tqdm(total=dataset_size, unit=\"MB\", unit_scale=True, desc=\"üì• Progression\") as pbar:\n",
    "            while process.poll() is None:\n",
    "                time.sleep(1)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # V√©rifier si le t√©l√©chargement a r√©ussi\n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ Dataset '{dataset_identifier}' t√©l√©charg√© avec succ√®s dans '{dataset_folder}'.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur lors du t√©l√©chargement. Code de retour: {process.returncode}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Erreur dans la commande Kaggle. Code de retour: {e.returncode}\")\n",
    "        print(f\"Erreur : {e.stderr.decode('utf-8')}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Erreur : La commande 'kaggle' n'a pas √©t√© trouv√©e. Installez l'API Kaggle.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Une erreur inattendue s'est produite : {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['id',  'title', 'ref', 'totalBytes', 'usabilityRating', \n",
    "          'description', 'url', 'lastUpdated', 'topicCount', \n",
    "          'voteCount', 'currentVersionNumber', 'tags',\n",
    "          'isPrivate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocean_datasets =list_datasets(keyword='finance', usability_min=0.6, min_mb=5,columns=columns)\n",
    "df_ocean_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ocean_datasets.columns)\n",
    "df_ocean_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_ocean_datasets.iterrows():\n",
    "\n",
    "    ref = row[\"ref\"]\n",
    "    download_dataset(dataset_ref=ref, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "metadata_list = []\n",
    "count = 0\n",
    "\n",
    "for idx, row in df_ocean_datasets.iterrows():\n",
    "    url = row[\"url\"]\n",
    "    url_list.append(url)\n",
    "    ref = row[\"ref\"]\n",
    "    \n",
    "    response = requests.get(f\"https://www.kaggle.com/api/v1/datasets/metadata/{ref}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.json()  # Stocker le JSON\n",
    "        metadata_list.append(content)  # Ajouter √† la liste des m√©tadonn√©es\n",
    "        \n",
    "        # V√©rifier si le dataset est priv√©\n",
    "        if content[\"info\"][\"isPrivate\"]:\n",
    "            df_ocean_datasets.drop(labels=idx, inplace=True)  # Supprimer la ligne\n",
    "            count += 1\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error fetching metadata for {url} (Status code: {response.status_code})\")\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_list)\n",
    "print(f\"Nombre de datasets priv√©s supprim√©s : {count}\")\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser la colonne \"info\" avant d'appliquer le filtrage des colonnes\n",
    "df_normalized = df_metadata.join(pd.json_normalize(df_metadata[\"info\"]))\n",
    "df_normalized.drop(columns=[\"info\"], inplace=True)\n",
    "\n",
    "# Appliquer le filtrage des colonnes apr√®s la normalisation\n",
    "df_normalized = df_normalized.loc[:, ~df_normalized.columns.str.contains('has|nullable', case=False)]\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_normalized.shape)\n",
    "df_normalized[\"isPrivate\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(keyword):\n",
    "    print(f\"Lister les mod√®les correspondant √† '{keyword}' :\")\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    models = api.models_list(search=keyword)\n",
    "    desired_columns = [\n",
    "        'id', 'ref', 'title', 'description', 'url',\n",
    "        'totalVotes', 'lastUpdated', 'tags', 'licenseName'\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for model in models:\n",
    "        model_dict = {key: model[key] for key in model if key in desired_columns}\n",
    "        result.append(model_dict)\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    # Suppression des colonnes inutiles contenant \"has\" ou \"Nullable\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('has|Nullable', case=False)]\n",
    "    df[\"tags\"] = df[\"tags\"].apply(lambda x: [d[\"name\"] for d in x] if x is not None else [])\n",
    "    # V√©rification des liens URL pour voir si les mod√®les sont accessibles\n",
    "    for idx, row in df.iterrows():\n",
    "        url = row[\"url\"]\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code >= 400:\n",
    "                df.drop(idx, inplace=True)\n",
    "        except requests.exceptions.RequestException:\n",
    "            df.drop(idx, inplace=True)\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list =[]\n",
    "\n",
    "for idx, row in df_ocean_datasets.iterrows():\n",
    "    url = row[\"url\"]\n",
    "    url_list.append(url)\n",
    "    title = row[\"title\"]\n",
    "    if title == 'OCEAN DATA / CLIMATE CHANGE / NASA':\n",
    "        try:\n",
    "            download_dataset(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else :\n",
    "        print('dataset not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plus_80 = df_ocean_datasets[df_ocean_datasets[\"usabilityRating\"]>0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plus_80.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
